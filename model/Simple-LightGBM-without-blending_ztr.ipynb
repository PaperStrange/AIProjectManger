{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 225961,
     "status": "ok",
     "timestamp": 1550621695021,
     "user": {
      "displayName": "章天任",
      "photoUrl": "https://lh5.googleusercontent.com/-hx7n9EqB1Xc/AAAAAAAAAAI/AAAAAAAAAA4/1QWBh2GXjJg/s64/photo.jpg",
      "userId": "16796019373879754656"
     },
     "user_tz": -480
    },
    "id": "kKt-d0e6MHUV",
    "outputId": "4b8d9921-65af-4a2f-ca9e-a66f5b442a33"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools \n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null \n",
    "!apt-get update -qq 2>&1 > /dev/null \n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse \n",
    "from google.colab import auth \n",
    "auth.authenticate_user() \n",
    "from oauth2client.client import GoogleCredentials \n",
    "creds = GoogleCredentials.get_application_default() \n",
    "import getpass \n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL \n",
    "vcode = getpass.getpass() \n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zekS_P2kMh3Z"
   },
   "outputs": [],
   "source": [
    "!mkdir -p driver\n",
    "!google-drive-ocamlfuse driver\n",
    "import os\n",
    "os.chdir(\"driver/loyalty_prediction_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12112,
     "status": "ok",
     "timestamp": 1550621712615,
     "user": {
      "displayName": "章天任",
      "photoUrl": "https://lh5.googleusercontent.com/-hx7n9EqB1Xc/AAAAAAAAAAI/AAAAAAAAAA4/1QWBh2GXjJg/s64/photo.jpg",
      "userId": "16796019373879754656"
     },
     "user_tz": -480
    },
    "id": "UKrQHmdZUJ6i",
    "outputId": "3e644d6c-f7ca-4dfd-9cee-fe3e4bdb007b"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7193,
     "status": "error",
     "timestamp": 1550494240730,
     "user": {
      "displayName": "章天任",
      "photoUrl": "https://lh5.googleusercontent.com/-hx7n9EqB1Xc/AAAAAAAAAAI/AAAAAAAAAA4/1QWBh2GXjJg/s64/photo.jpg",
      "userId": "16796019373879754656"
     },
     "user_tz": -480
    },
    "id": "c-kE26_kMlVi",
    "outputId": "e346cff6-2ca7-4b37-be07-afa099d0b2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 201917, test samples: 123623\n",
      "Memory usage after optimization is: 16.14 MB\n",
      "Decreased by 67.5%\n",
      "train & test - done in 6s\n",
      "[Output] expected to be end in 5 runs, this is 1 run...\n",
      "Memory usage after optimization is: 406.27 MB\n",
      "Decreased by 40.8%\n",
      "[Output] expected to be end in 5 runs, this is 2 run...\n",
      "Memory usage after optimization is: 406.27 MB\n",
      "Decreased by 40.8%\n",
      "[Output] expected to be end in 5 runs, this is 3 run...\n",
      "Memory usage after optimization is: 406.27 MB\n",
      "Decreased by 40.8%\n",
      "[Output] expected to be end in 5 runs, this is 4 run...\n",
      "Memory usage after optimization is: 406.27 MB\n",
      "Decreased by 40.8%\n",
      "[Output] expected to be end in 5 runs, this is 5 run...\n",
      "Memory usage after optimization is: 346.16 MB\n",
      "Decreased by 40.8%\n",
      "Memory usage after optimization is: 1776.88 MB\n",
      "Decreased by 53.3%\n",
      "processed 100/325540 unique card ids.\n",
      "processed 200/325540 unique card ids.\n",
      "processed 300/325540 unique card ids.\n",
      "processed 400/325540 unique card ids.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc, sys\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0']\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "    \n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "    \n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ADDITIONAL MONTHLY PURCHASE AMOUNT RATIO FEATURE\n",
    "def purchase_amount_ratio(df):\n",
    "    from collections import defaultdict\n",
    "    unique_card_ids = df['card_id'].drop_duplicates()\n",
    "    n = len(unique_card_ids)\n",
    "    feature_df = pd.DataFrame({'card_id': unique_card_ids,\n",
    "                               'monthly_purchase_amount_ratio': np.zeros(len(unique_card_ids))})\n",
    "    for j, card_id in enumerate(unique_card_ids):\n",
    "        card_df = df.loc[df['card_id'] == card_id, :]\n",
    "        merchant_ids = df.loc[df['card_id'] == card_id, 'merchant_id']\n",
    "        ratio = []\n",
    "#         print(len(merchant_ids))\n",
    "        for merchant in merchant_ids:\n",
    "            purchase_amount = card_df.loc[card_df['merchant_id'] == merchant, 'purchase_amount']\n",
    "            month_lag = card_df.loc[card_df['merchant_id'] == merchant, 'month_lag']\n",
    "            if len(purchase_amount):\n",
    "                unique_month_lag = month_lag.drop_duplicates()\n",
    "                pa_dict = defaultdict(float)\n",
    "                for i in range(len(purchase_amount)):\n",
    "                    pa_dict[month_lag.values[i]] += purchase_amount.values[i]\n",
    "                m_ratio = []\n",
    "                s_lag = np.sort(unique_month_lag)\n",
    "                for i in range(len(unique_month_lag) - 1):\n",
    "                    m_ratio.append(np.power(pa_dict[s_lag[i+1]] / pa_dict[s_lag[i]], 1.0 / (s_lag[i+1] - s_lag[i])))\n",
    "                if len(m_ratio):\n",
    "                    ratio.append(np.mean(m_ratio))\n",
    "        if len(ratio):\n",
    "            feature_df.loc[feature_df['card_id'] == card_id, 'monthly_purchase_amount_ratio'] = np.mean(ratio)\n",
    "        if (j+1) % 100 == 0 or j+1 == n:\n",
    "            print('processed {}/{} unique card ids.'.format(j+1, n))\n",
    "                \n",
    "    print(feature_df)\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv('input/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv('input/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "    \n",
    "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
    "    df.reset_index(inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# preprocessing historical transactions\n",
    "def historical_transactions(num_rows=None):\n",
    "    # load csv\n",
    "    hist_reader = pd.read_csv('processed/historical_transactions.csv', chunksize=6000000)\n",
    "    hist_list = []\n",
    "    indent = 1\n",
    "    for each_chunk in hist_reader:\n",
    "        print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
    "        each_chunk = reduce_mem_usage(each_chunk)\n",
    "        hist_list.append(each_chunk)\n",
    "        indent += 1\n",
    "        gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "    hist_df = pd.concat(hist_list, ignore_index=True)\n",
    "    del hist_list\n",
    "    del each_chunk\n",
    "    del hist_reader\n",
    "\n",
    "    # fillna\n",
    "    hist_df['category_2'].fillna(1.0, inplace=True)\n",
    "    hist_df['category_3'].fillna('A', inplace=True)\n",
    "    hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    # hist_df['merchant_id'].fillna('', inplace=True)\n",
    "    hist_df['installments'].replace(-1, np.nan, inplace=True)\n",
    "    hist_df['installments'].replace(999, np.nan, inplace=True)\n",
    "\n",
    "#     # Re-construct 2019/2/17: fill vacancy merchant id by other method [here](https://www.kaggle.com/raddar/merchant-id-imputations)\n",
    "#     new_merchant_df = pd.read_csv(data_new_trans_path, nrows=num_rows).fillna('')\n",
    "#     fields = ['card_id','city_id','category_1','installments','category_3',\\\n",
    "#           'merchant_category_id','category_2','state_id','subsector_id']\n",
    "#     new_merchant_df_part = new_merchant_df[fields + ['merchant_id']].drop_duplicates()\n",
    "#     new_merchant_df_part = new_merchant_df_part.loc[new_merchant_df_part['merchant_id'] != '']  \n",
    "#     # take only unique merchants for the `fields` combination\n",
    "#     uq_new_merchants = new_merchant_df_part.groupby(fields)['merchant_id'].count().reset_index(name = 'n_merchants')\n",
    "#     uq_new_merchants = uq_new_merchants.loc[uq_new_merchants['n_merchants'] == 1]\n",
    "#     uq_new_merchants = uq_new_merchants.merge(new_merchant_df_part, on=fields)\n",
    "#     uq_new_merchants.drop('n_merchants', axis=1, inplace=True)\n",
    "#     # rename the merchant_id so we can join it more easily later on\n",
    "#     uq_new_merchants.columns = fields + ['imputed_merchant_id']\n",
    "#     del new_merchant_df\n",
    "#     del new_merchant_df_part\n",
    "#     hist_df = hist_df.merge(uq_new_merchants, on = fields, how = 'left')\n",
    "#     # make the actual imputation for the merchant_id field\n",
    "#     hist_df.loc[(hist_df['merchant_id']=='') & (~pd.isnull(hist_df['imputed_merchant_id'])), 'merchant_id'] = \\\n",
    "#         hist_df.loc[(hist_df['merchant_id']=='') & (~pd.isnull(hist_df['imputed_merchant_id'])), 'imputed_merchant_id']\n",
    "    \n",
    "#     # Re-construct 2019/2/17: create new feature from \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
    "#     hist_df[\"purchase_amount_new\"] = np.round(hist_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
    "    \n",
    "    # trim\n",
    "#     hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    hist_df['purchase_amount'] = np.round(hist_df['purchase_amount'] / 0.00150265118 + 497.06, 2)\n",
    "    # Y/N to 1/0\n",
    "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # datetime features\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
    "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
    "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
    "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
    "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
    "    \n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    \n",
    "    # ADDITIONAL PURCHASE AMOUNT RATIO FEATURES\n",
    "    feature_df = purchase_amount_ratio(hist_df)\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    hist_df['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Mothers Day: May 14 2017\n",
    "    hist_df['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #fathers day: August 13 2017\n",
    "    hist_df['fathers_day_2017'] = (pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    hist_df['Children_day_2017'] = (pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Valentine's Day : 12th June, 2017\n",
    "    hist_df['Valentine_Day_2017'] = (pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    hist_df['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #2018\n",
    "    #Mothers Day: May 13 2018\n",
    "    hist_df['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
    "    hist_df['month_diff'] += hist_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    hist_df['duration'] = hist_df['purchase_amount']*hist_df['month_diff']\n",
    "    hist_df['amount_month_ratio'] = hist_df['purchase_amount']/hist_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "       \n",
    "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
    "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
    "\n",
    "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max']-hist_df['hist_purchase_date_min']).dt.days\n",
    "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff']/hist_df['hist_card_id_size']\n",
    "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
    "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "\n",
    "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
    "    hist_df.reset_index(inplace=True)\n",
    "    \n",
    "    # ADDITIONAL PURCHASE AMOUNT RATIO FEATURES\n",
    "    hist_df = pd.merge(hist_df, feature_df, on='card_id', how='outer')\n",
    "    \n",
    "    return hist_df\n",
    " \n",
    "    \n",
    "# preprocessing new_merchant_transactions\n",
    "def new_merchant_transactions(num_rows=None):\n",
    "    # load csv\n",
    "    new_merchant_df = pd.read_csv('input/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "\n",
    "    # fillna\n",
    "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
    "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
    "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "#     # Re-construct 2019/2/17: create new feature from \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
    "#     new_merchant_df[\"purchase_amount_new\"] = np.round(new_merchant_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
    "    \n",
    "    # trim\n",
    "#     new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    new_merchant_df[\"purchase_amount\"] = np.round(new_merchant_df[\"purchase_amount\"] / 0.00150265118 + 497.06, 2)\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
    "\n",
    "    # datetime features\n",
    "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
    "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
    "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
    "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
    "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
    "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
    "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']\n",
    "    \n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    \n",
    "    # ADDITIONAL PURCHASE AMOUNT RATIO FEATURES\n",
    "    feature_df = purchase_amount_ratio(new_merchant_df)\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #Mothers Day: May 13 2018\n",
    "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
    "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['duration'] = new_merchant_df['purchase_amount']*new_merchant_df['month_diff']\n",
    "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount']/new_merchant_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['weekend'] = ['mean']\n",
    "    aggs['month'] = ['mean', 'min', 'max']\n",
    "    aggs['weekday'] = ['mean', 'min', 'max']\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
    "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
    "\n",
    "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max']-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff']/new_merchant_df['new_card_id_size']\n",
    "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
    "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    gc.collect()\n",
    "    sys._clear_type_cache()\n",
    "\n",
    "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
    "    new_merchant_df.reset_index(inplace=True)\n",
    "    \n",
    "    # ADDITIONAL PURCHASE AMOUNT RATIO FEATURES\n",
    "    new_merchant_df = pd.merge(new_merchant_df, hist_df, on='card_id', how='outer')\n",
    "        \n",
    "    return new_merchant_df\n",
    "\n",
    "\n",
    "# additional features\n",
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "    \n",
    "    return best_features\n",
    "\n",
    "    \n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def kfold_lightgbm(train_df, test_df, num_folds, stratified = False, debug= False):\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    \n",
    "#     # Re-construct 2019/2/17: create dic to store loss\n",
    "#     loss_train, loss_valid = {each_fold: [] for each_fold in range(num_folds)}, {each_fold: [] for each_fold in range(num_folds)}\n",
    "    \n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,\n",
    "                                label=train_y,\n",
    "                                free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,\n",
    "                               label=valid_y,\n",
    "                               free_raw_data=False)\n",
    "\n",
    "        # params optimized by optuna\n",
    "        params ={\n",
    "                'task': 'train',\n",
    "                'boosting': 'goss',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'learning_rate': 0.01,\n",
    "                'subsample': 0.9855232997390695,\n",
    "                'max_depth': 7,\n",
    "                'top_rate': 0.9064148448434349,\n",
    "                'num_leaves': 63,\n",
    "                'min_child_weight': 41.9612869171337,\n",
    "                'other_rate': 0.0721768246018207,\n",
    "                'reg_alpha': 9.677537745007898,\n",
    "                'colsample_bytree': 0.5665320670155495,\n",
    "                'min_split_gain': 9.820197773625843,\n",
    "                'reg_lambda': 8.2532317400459,\n",
    "                'min_data_in_leaf': 21,\n",
    "                'verbose': -1,\n",
    "                'seed':int(2**n_fold),\n",
    "                'bagging_seed':int(2**n_fold),\n",
    "                'drop_seed':int(2**n_fold)\n",
    "                }\n",
    "\n",
    "        reg = lgb.train(\n",
    "                        params,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_test],\n",
    "                        valid_names=['train', 'test'],\n",
    "                        num_boost_round=10000,\n",
    "                        early_stopping_rounds= 200,\n",
    "                        verbose_eval=100\n",
    "                        )\n",
    "\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "        gc.collect()\n",
    "        sys._clear_type_cache()\n",
    "        \n",
    "#     # Re-construct 2019/2/17: store two kinds of loss        \n",
    "#         loss_train[n_fold].append(rmse(oof_preds[train_idx], train_y[train_idx]))\n",
    "#         loss_valid[n_fold].append(rmse(oof_preds[valid_idx], valid_y[valid_idx]))\n",
    "    \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # display importances\n",
    "    best_features_df = display_importances(feature_importance_df)\n",
    "\n",
    "    if not debug:\n",
    "        # save submission file\n",
    "        test_df.loc[:,'target'] = sub_preds\n",
    "        test_df = test_df.reset_index()\n",
    "        test_df[['card_id', 'target']].to_csv(submission_file_name, index=False)\n",
    "        \n",
    "    return best_features_df\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    with timer(\"train & test\"):\n",
    "        df = train_test(num_rows)   \n",
    "    with timer(\"historical transactions\"):\n",
    "        df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')    \n",
    "    with timer(\"new merchants\"):\n",
    "        df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')      \n",
    "    with timer(\"additional features\"):\n",
    "        df = additional_features(df)\n",
    "#     df.to_csv('data_aggregated.csv', index=False)\n",
    "    with timer(\"split train & test\"):\n",
    "        train_df = df[df['target'].notnull()]\n",
    "        test_df = df[df['target'].isnull()]\n",
    "        del df\n",
    "        gc.collect()   \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        best_features_df = kfold_lightgbm(train_df, test_df, num_folds=11, stratified=False, debug=debug)\n",
    "        \n",
    "#     with timer(\"Evaluate modeling results\"):\n",
    "#         best_features_df.set_index(\"fold\", inplace=True)\n",
    "#         best_features_df.loc[2, :].sort_values(by=\"importance\",ascending=False)\n",
    "        \n",
    "#         loss_train_df = pd.DataFrame(loss_train)\n",
    "#         loss_valid_df = pd.DataFrame(loss_valid)\n",
    "#         plt.plot(np.arange(loss_train_df.shape[1]), loss_train_df.loc[0, :].values, \"bo-\", np.arange(loss_valid_df.shape[1]), loss_valid_df.loc[0, :].values, \"ro-\");\n",
    "      \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main(debug=False)\n",
    "#         main(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "loyalty_Simple-LightGBM-without-blending.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
