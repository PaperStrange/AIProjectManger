{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_valid_Gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "HROQn0rPTUkV",
        "fyR6YM9lTUkX",
        "A1gNQ2rZUPsW",
        "BN3QzRJTVB-U",
        "d4zFm5T0VB-Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "cvZKIS1mcwxe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Author: hzq\n",
        "# Last review date: 2019/3/24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cE4orxziyUkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basic codes"
      ]
    },
    {
      "metadata": {
        "id": "eA7OKZQmaYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools \n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null \n",
        "!apt-get update -qq 2>&1 > /dev/null \n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse \n",
        "from google.colab import auth \n",
        "auth.authenticate_user() \n",
        "from oauth2client.client import GoogleCredentials \n",
        "creds = GoogleCredentials.get_application_default() \n",
        "import getpass \n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL \n",
        "vcode = getpass.getpass() \n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UmQsHbPNab7z",
        "colab_type": "code",
        "outputId": "7f8e064a-d371-48c9-89b1-91f074dbca72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p driver\n",
        "!google-drive-ocamlfuse driver\n",
        "import os\n",
        "\n",
        "os.chdir(\"driver/Colab Notebooks/Santander_Cus_ Trans_kaggle/\")\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "catboost_info\t\t    generator.json\t      requirements.txt\n",
            "data\t\t\t    generator_weights.hdf5    submit\n",
            "discriminator.json\t    infogan_hzq_20190319.csv  timeline\n",
            "discriminator_weights.hdf5  lgbm_importances.png      train_loss_collector.csv\n",
            "download\t\t    LICENSE\t\t      utils\n",
            "FI_20190315.png\t\t    model\t\t      valid_loss_collector.csv\n",
            "FI.png\t\t\t    README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GuZK1CBqbNeV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fU5XEh6kSAGX"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Parts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X9I0WLhYdEqS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "#   print('ERROR: Not connected to a TPU runtime')\n",
        "# else:\n",
        "#   tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#   print ('TPU address is', tpu_address)\n",
        "#   with tf.Session(tpu_address) as session:\n",
        "#     devices = session.list_devices()\n",
        "#   print('TPU devices:')\n",
        "#   pprint.pprint(devices)\n",
        "\n",
        "if 'COLAB_GPU' not in os.environ:\n",
        "    print('ERROR: Not connected to a GPU runtime')\n",
        "else: \n",
        "    print('GPU devices:')\n",
        "    pprint.pprint(tf.test.gpu_device_name())\n",
        "print('CPU devices:')\n",
        "pprint.pprint(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rDgUFnuadGvI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !cat /proc/meminfo\n",
        "# !cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cz0IoFwotC5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import required packages"
      ]
    },
    {
      "metadata": {
        "id": "BLugmmWjmPNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random, copy, sys, gc, os  # sys.getsizeof()\n",
        "import logging\n",
        "import datetime\n",
        "import warnings\n",
        "import functools\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display # Allows the use of display() for DataFrames\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bWc8qU7SKEd9",
        "outputId": "0c4acbab-23f0-40e4-f829-f41afb4894c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, concatenate\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding1D, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling1D, Conv1D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "KT54rEO6otC-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## prepared process\n"
      ]
    },
    {
      "metadata": {
        "id": "xxS2jWEcpkyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### read data online"
      ]
    },
    {
      "metadata": {
        "id": "zAX08MdwotDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### read data offline"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w-zO8dX2dZBK",
        "outputId": "cb08b1e8-cd77-41a8-838d-d283ad989d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_df = pd.read_csv(\"./data/data_raw/train/train.csv\")\n",
        "test_df = pd.read_csv(\"./data/data_raw/test/test.csv\")\n",
        "\n",
        "train_df = reduce_mem_usage(train_df)\n",
        "test_df = reduce_mem_usage(test_df)\n",
        "\n",
        "idx = features = test_df.columns.values[1:202]\n",
        "for df in [test_df, train_df]:\n",
        "# for df in [test_df, X_train_add_df]:\n",
        "    df['sum'] = df[idx].sum(axis=1)  \n",
        "    df['min'] = df[idx].min(axis=1)\n",
        "    df['max'] = df[idx].max(axis=1)\n",
        "    df['mean'] = df[idx].mean(axis=1)\n",
        "    df['std'] = df[idx].std(axis=1)\n",
        "    df['skew'] = df[idx].skew(axis=1)\n",
        "    df['kurt'] = df[idx].kurtosis(axis=1)\n",
        "    df['med'] = df[idx].median(axis=1)\n",
        "\n",
        "X_train_nullcount = train_df.isnull().mean()    \n",
        "X_train_nullcount[X_train_nullcount!=0]\n",
        "# X_train_add_nullcount = X_train_add_df.isnull().mean()    \n",
        "# X_train_add_nullcount[X_train_add_nullcount!=0]\n",
        "\n",
        "\n",
        "# Re-construct 2019/3/14: deal with the imbalance by over-sampling\n",
        "\n",
        "# # !pip install imblearn\n",
        "# from collections import Counter\n",
        "# from imblearn.over_sampling import SMOTE \n",
        "\n",
        "\n",
        "# train_df_part = train_df.drop([\"ID_code\"], axis=1)\n",
        "# X_train = train_df_part.drop([\"target\"], axis=1)\n",
        "# y_train = train_df_part.loc[:, \"target\"]\n",
        "\n",
        "# print('Original dataset shape {}'.format(Counter(y_train)))\n",
        "# sm = SMOTE(random_state=42)\n",
        "# X_train_add_arr, y_train_add_arr = sm.fit_sample(X_train, y_train)\n",
        "# print('Resampled dataset shape {}'.format(Counter(y_train_add_arr)))\n",
        "\n",
        "# X_train_add_df = pd.DataFrame(X_train_add_arr, columns=X_train.columns)\n",
        "# y_train_add_df = pd.Series(y_train_add_arr)\n",
        "\n",
        "# del train_df_part, X_train, y_train, X_train_add_arr, y_train_add_arr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 78.01 Mb (74.7% reduction)\n",
            "Mem. usage decreased to 77.82 Mb (74.6% reduction)\n",
            "CPU times: user 1min 6s, sys: 2.18 s, total: 1min 8s\n",
            "Wall time: 1min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqHe0gzUcBcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note 2019/3/14: this is a work in progress, some of the features added here will be later dropped\n",
        "\n",
        "# features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
        "# for feature in features:\n",
        "#     train_df['r2_'+feature] = np.round(train_df[feature], 2)\n",
        "#     test_df['r2_'+feature] = np.round(test_df[feature], 2)\n",
        "#     train_df['r1_'+feature] = np.round(train_df[feature], 1)\n",
        "#     test_df['r1_'+feature] = np.round(test_df[feature], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oUWCltrIb2fS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train_add_df.drop([\"kurt\"], axis=1,inplace=True) \n",
        "train_df.drop([\"kurt\"], axis=1,inplace=True) \n",
        "\n",
        "# features = [c for c in X_train_add_df.columns if c not in ['ID_code', 'target']]\n",
        "features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
        "target = train_df['target']\n",
        "\n",
        "# del train_df\n",
        "# X_train_add_df = reduce_mem_usage(X_train_add_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ejZzpKvzeSIx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(train_df[features], target, test_size=0.1, shuffle=True)\n",
        "X_test = test_df[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jNr8U8-Sb6P6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "sys._clear_type_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sZV0-_qUqU3O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###model: [InfoGan](https://github.com/eriklindernoren/Keras-GAN/blob/master/infogan/infogan.py)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HROQn0rPTUkV"
      },
      "cell_type": "markdown",
      "source": [
        "#### constructs model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QI1ZWJgBd987",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 2019/3/21: add docstrings following google style!\n",
        "\n",
        "class INFOGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = len(features)\n",
        "        self.num_classes = 2\n",
        "        self.img_shape = (self.img_rows, )\n",
        "        self.latent_dim = 200\n",
        "        self.loss_name = ['d_loss', 'q_loss', 'g_loss'] \n",
        "        optimizer = Adam(0.002, 0.5)\n",
        "        losses = ['binary_crossentropy', self.mutual_info_loss]\n",
        "\n",
        "        # Build and the discriminator and recognition network\n",
        "        \n",
        "        self.discriminator, self.auxilliary = self.build_disk_and_q_net()\n",
        "\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the recognition network Q\n",
        "        \n",
        "        self.auxilliary.compile(loss=[self.mutual_info_loss],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        \n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        \n",
        "        gen_input = Input(shape=(self.latent_dim, ))    \n",
        "        img = self.generator(gen_input)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        \n",
        "        self.discriminator.trainable = False\n",
        "        valid = self.discriminator(img)\n",
        "        \n",
        "        # The recognition network produces the label\n",
        "        \n",
        "        target_label = self.auxilliary(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        \n",
        "        self.combined = Model(gen_input, [valid, target_label])\n",
        "        self.combined.compile(loss=losses,\n",
        "            optimizer=optimizer)\n",
        "        \n",
        "    @functools.lru_cache(maxsize=128)\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(500, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(300))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(self.img_rows))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "        gen_input = Input(shape=(self.latent_dim, ))\n",
        "        img = model(gen_input)\n",
        "        # print(\"The generator model structure is: \")\n",
        "        # model.summary()\n",
        "        return Model(gen_input, img)\n",
        "\n",
        "    @functools.lru_cache(maxsize=128)\n",
        "    def build_disk_and_q_net(self):\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        \n",
        "        # Shared layers between discriminator and recognition network\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Dense(300))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(500))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(700))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(800))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        img_embedding = model(img)\n",
        "\n",
        "        # Discriminator\n",
        "        \n",
        "        validity = Dense(1, activation='sigmoid')(img_embedding)\n",
        "        # validity = Dense(1, activation='relu')(img_embedding)\n",
        "        # print(\"The discriminator model structure is: \")        \n",
        "        # model.summary()\n",
        "        \n",
        "        # Recognition\n",
        "        \n",
        "        q_net = Dense(128, activation='relu')(img_embedding)\n",
        "        label = Dense(self.num_classes, activation='softmax')(q_net)\n",
        "        # print(\"The recognition model structure is: \")        \n",
        "        # model.summary()\n",
        "        # Return discriminator and recognition network\n",
        "        \n",
        "        return Model(img, validity), Model(img, label)\n",
        "\n",
        "    @functools.lru_cache(maxsize=128)\n",
        "    def mutual_info_loss(self, c, c_given_x):\n",
        "      \n",
        "        \"\"\"The mutual information metric we aim to minimize\"\"\"\n",
        "        \n",
        "        eps = 1e-8\n",
        "        conditional_entropy = K.mean(- K.sum(K.log(c_given_x + eps) * c, axis=1))\n",
        "        entropy = K.mean(- K.sum(K.log(c + eps) * c, axis=1))\n",
        "        return conditional_entropy + entropy\n",
        "    \n",
        "    @functools.lru_cache(maxsize=128)\n",
        "    def sample_generator_input(self, batch_size):\n",
        "        sampled_noise = np.random.normal(0, 1, (batch_size, self.latent_dim-self.num_classes))\n",
        "        sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
        "        sampled_labels = to_categorical(sampled_labels, num_classes=self.num_classes)\n",
        "        return sampled_noise, sampled_labels    \n",
        "    \n",
        "    @functools.lru_cache(maxsize=128)\n",
        "    def train(self, epochs, batch_size=500):              \n",
        "        # valid = np.ones((batch_size, ))  # Adversarial ground truths\n",
        "        fake = np.zeros((batch_size, ))  # Adversarial ground truths\n",
        "        self.loss_list_train = []\n",
        "        self.loss_list_valid = []\n",
        "        # acc, precision, recall, f1_score = 0, 0, 0, 0\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------          \n",
        "            idx = np.random.choice(X_train.index, batch_size)\n",
        "            idx_ = [np.where(np.array(X_train.index)==eachidx)[0][0] for eachidx in idx]\n",
        "            idx_dict[epoch] = idx_\n",
        "            imgs = X_train.iloc[idx_][:]\n",
        "            valid_t = np.array([float(y_train.iloc[eachidx]) for eachidx in idx_])\n",
        "            \n",
        "            # Sample noise and categorical labels\n",
        "            \n",
        "            # labels_f = np.array([1.0 if label > 0.5 else 0 for label in np.random.uniform(0, 1, size=len(data_valid))])\n",
        "            # labels_f = np.array([1.0 if random.uniform(0, 1) > 0.5 else 0.0 for indent in range(len(data_valid))])\n",
        "            # sampled_noise, sampled_noise_valid, sampled_labels, sampled_labels_valid = train_test_split(data_valid, labels_f, train_size=batch_size, random_state=23)\n",
        "            # sampled_noise, sampled_labels = sampled_noise.iloc[:, :self.latent_dim-2], to_categorical(sampled_labels.reshape(-1, 1), num_classes=self.num_classes)\n",
        "            sampled_noise, sampled_labels = self.sample_generator_input(batch_size)\n",
        "            gen_input = np.concatenate((sampled_noise, sampled_labels), axis=1)\n",
        "            \n",
        "            # Generate  new images\n",
        "            \n",
        "            gen_imgs = self.generator.predict(gen_input)\n",
        "            \n",
        "            # Train on real and generated data\n",
        "            \n",
        "            self.discriminator.trainable = True\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid_t)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "            # Avg. loss\n",
        "            \n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator and Q-network\n",
        "            # ---------------------\n",
        "            self.discriminator.trainable = False\n",
        "            g_loss = self.combined.train_on_batch(gen_input, [valid_t, sampled_labels]) \n",
        "            print (\"Display train loss: %d [D loss: %.4f, acc.: %.4f%%] [Q loss: %.4f] [G loss: %.4f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[1], g_loss[2]))\n",
        "            self.loss_list_train.append([d_loss[0], g_loss[1], g_loss[2]])\n",
        "\n",
        "            # ---------------------\n",
        "            #  Calculate validation loss\n",
        "            # ---------------------           \n",
        "            idx_valid = np.random.choice(X_valid.index, batch_size)\n",
        "            idx_valid_ = [np.where(np.array(X_valid.index)==eachidx)[0][0] for eachidx in idx_valid]\n",
        "            imgs_valid = X_valid.iloc[idx_valid_][:]\n",
        "            valid_t_valid = np.array([float(y_valid.iloc[eachidx]) for eachidx in idx_valid_])\n",
        "            \n",
        "            # sampled_noise_valid, sampled_labels_valid = sampled_noise_valid.iloc[:batch_size, :self.latent_dim-2], to_categorical(sampled_labels_valid[:batch_size].reshape(-1, 1), num_classes=self.num_classes)\n",
        "            sampled_noise_valid, sampled_labels_valid = self.sample_generator_input(batch_size)\n",
        "            gen_input_valid = np.concatenate((sampled_noise_valid, sampled_labels_valid), axis=1)\n",
        "            \n",
        "            gen_imgs_valid = self.generator.predict(gen_input_valid)   \n",
        "            \n",
        "            d_loss_real_valid = self.discriminator.test_on_batch(imgs_valid, valid_t_valid)\n",
        "            d_loss_fake_valid = self.discriminator.test_on_batch(gen_imgs_valid, fake)\n",
        "            \n",
        "            d_loss_valid = 0.5 * np.add(d_loss_real_valid, d_loss_fake_valid)\n",
        "            \n",
        "            g_loss_valid = self.combined.test_on_batch(gen_input_valid, [valid_t_valid, sampled_labels_valid])        \n",
        "            # print (\"Display valid loss: %d [D loss: %.4f, acc.: %.4f%%] [Q loss: %.4f] [G loss: %.4f]\" % (epoch, d_loss_valid[0], 100*d_loss_valid[1], g_loss_valid[1], g_loss_valid[2]))\n",
        "            self.loss_list_valid.append([d_loss_valid[0], g_loss_valid[1], g_loss_valid[2]])\n",
        "\n",
        "            # acc, precision, recall, f1_score = \\\n",
        "                # acc + accuracy_score(valid_t_valid, self.labels_test_pred), \\\n",
        "                # precision + precision_score(valid_t_valid, self.labels_test_pred), \\\n",
        "                # recall + recall_score(valid_t_valid, self.labels_test_pred), \\\n",
        "               #  f1_score + f1_score(valid_t_valid, self.labels_test_pred)\n",
        "    # print('Accuracy of the network on the ? validation data: %.3f %%' % (100*acc/epochs))\n",
        "    # print('Precision of the network on the ? validation data: %.3f %%' % (100*precision/epochs))\n",
        "    # print('Recall of the network on the ? validation data: %.3f %%' % (100*recall/epochs))\n",
        "    # print('F1-score of the network on the ? validation data: %.3f %%' % (100*f1_score/epochs))\n",
        "\n",
        "# TODO 2019/3/19: check the formation of prediction    \n",
        "#     def predict(self):\n",
        "#         return self.discriminator.predict(X_test)\n",
        "        \n",
        "    def save_model(self):\n",
        "        def save(model, model_name):\n",
        "            model_path = \"%s.json\" % model_name\n",
        "            weights_path = \"%s_weights.hdf5\" % model_name\n",
        "            options = {\"file_arch\": model_path,\n",
        "                        \"file_weight\": weights_path}\n",
        "            json_string = model.to_json()\n",
        "            open(options['file_arch'], 'w').write(json_string)\n",
        "            model.save_weights(options['file_weight'])\n",
        "        save(self.generator, \"generator\")\n",
        "        save(self.discriminator, \"discriminator\")\n",
        "        # save(self.auxilliary, 'recognition')\n",
        "      \n",
        "    @functools.lru_cache(maxsize=128) \n",
        "    def save_loss(self):\n",
        "        loss_df = pd.DataFrame(columns=list(self.loss_name), data=self.loss_list_train)\n",
        "        loss_df.to_csv('train_loss_collector.csv', index=False)\n",
        "        del  self.loss_list_train\n",
        "#         loss_df = pd.DataFrame(columns=list(self.loss_name), data=self.loss_list_valid)\n",
        "#         loss_df.to_csv('valid_loss_collector.csv', index=False)\n",
        "#         del  self.loss_list_valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fyR6YM9lTUkX"
      },
      "cell_type": "markdown",
      "source": [
        "#### evaluate model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "59FSgqdozXpw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 500\n",
        "epochs_num = 300\n",
        "idx_dict = {}\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    X_train, X_valid, y_train, y_valid = \\\n",
        "        train_test_split(\n",
        "                train_df[features], target, \n",
        "                train_size=int(0.8*(train_df.shape[0])), \n",
        "                random_state=23)\n",
        "    infogan = INFOGAN()\n",
        "    infogan.train(epochs=epochs_num, batch_size=batch_size)\n",
        "    infogan.save_loss()\n",
        "    infogan.save_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMezqybyaRlT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_train = pd.read_csv('train_loss_collector.csv')\n",
        "loss_valid = pd.read_csv('valid_loss_collector.csv')\n",
        "\n",
        "plt.plot()\n",
        "plt.plot(np.arange(epochs_num), loss_train['d_loss'], 'r-', label='train')\n",
        "plt.plot(np.arange(epochs_num), loss_valid['d_loss'], 'b-', label='valid')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H6HSv5Y7CaE2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_train.plot();\n",
        "# loss_train.plot(kind='bar', stacked=True);\n",
        "loss_data.div(loss_data.sum(axis=1), axis=0).plot(kind='barh', stacked=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RX8Xq_z958IP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_valid.plot();\n",
        "# loss_valid.plot(kind='bar', stacked=True);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jjgcd8uJOIxN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "sys._clear_type_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1gNQ2rZUPsW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### submit prediction"
      ]
    },
    {
      "metadata": {
        "id": "RrlSknpg9DW1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_predict = infogan.discriminator.predict(test_df[features])\n",
        "\n",
        "submit = pd.DataFrame(test_df['ID_code'])\n",
        "submit['target'] = y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ETi2Jwb1mW76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit['target'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fr9oMp7_pmb_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit['target'][submit['target'] > 0.1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oq6ELe4BpVCR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit.to_csv(\"infogan_hzq_20190319.csv\", index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hznC5B1jVB-P"
      },
      "cell_type": "markdown",
      "source": [
        "###model: [TripleGan](https://github.com/taki0112/TripleGAN-Tensorflow)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "P427Vo7aVB-Q"
      },
      "cell_type": "markdown",
      "source": [
        "#### constructs model"
      ]
    },
    {
      "metadata": {
        "id": "k9G4cCnsfhHX",
        "colab_type": "code",
        "outputId": "d05a8077-4789-4c1d-b719-181ab172b765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install ops"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ops\n",
            "  Downloading https://files.pythonhosted.org/packages/56/40/24e083823c39b485cb5473e62124e9c38cc0fce10f075d3189acf173b56f/ops-0.4.7.tar.gz\n",
            "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-sgnc5shc/ops/\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVEkvfr2feYg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import cifar10\n",
        "from ops import *\n",
        "from utils import *\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aT1cF1uITUkW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 2019/3/21: add docstrings following google style!\n",
        "# TODO 2019/3/21: apply modification to use it in numerical classification task\n",
        "\n",
        "class TripleGAN(object) :\n",
        "    def __init__(self, sess, epoch, batch_size, unlabel_batch_size, z_dim, dataset_name, n, gan_lr, cla_lr, checkpoint_dir, result_dir, log_dir):\n",
        "        self.sess = sess\n",
        "        self.dataset_name = dataset_name\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.log_dir = log_dir\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.unlabelled_batch_size = unlabel_batch_size\n",
        "        self.test_batch_size = 500\n",
        "        self.model_name = \"TripleGAN\"  # name for checkpoint\n",
        "        \n",
        "        if self.dataset_name == 'cifar10' :\n",
        "            self.input_height = 32\n",
        "            self.input_width = 32\n",
        "            self.output_height = 32\n",
        "            self.output_width = 32\n",
        "\n",
        "            self.z_dim = z_dim\n",
        "            self.y_dim = 10\n",
        "            self.c_dim = 3\n",
        "\n",
        "            self.learning_rate = gan_lr # 3e-4, 1e-3\n",
        "            self.cla_learning_rate = cla_lr # 3e-3, 1e-2 ?\n",
        "            self.GAN_beta1 = 0.5\n",
        "            self.beta1 = 0.9\n",
        "            self.beta2 = 0.999\n",
        "            self.epsilon = 1e-8\n",
        "            self.alpha = 0.5\n",
        "            self.alpha_cla_adv = 0.01\n",
        "            self.init_alpha_p = 0.0 # 0.1, 0.03\n",
        "            self.apply_alpha_p = 0.1\n",
        "            self.apply_epoch = 200 # 200, 300\n",
        "            self.decay_epoch = 50\n",
        "\n",
        "            self.sample_num = 64\n",
        "            self.visual_num = 100\n",
        "            self.len_discrete_code = 10\n",
        "\n",
        "            self.data_X, self.data_y, self.unlabelled_X, self.unlabelled_y, self.test_X, self.test_y = cifar10.prepare_data(n) # trainX, trainY, testX, testY\n",
        "\n",
        "            self.num_batches = len(self.data_X) // self.batch_size\n",
        "\n",
        "        else :\n",
        "#             raise NotImplementedError\n",
        "            self.input_height = 1\n",
        "            self.input_width = len(features)\n",
        "#             self.output_height = 32\n",
        "#             self.output_width = 32\n",
        "\n",
        "            self.z_dim = z_dim\n",
        "            self.y_dim = 2\n",
        "#             self.c_dim = 3\n",
        "\n",
        "            self.learning_rate = gan_lr # 3e-4, 1e-3\n",
        "            self.cla_learning_rate = cla_lr # 3e-3, 1e-2 ?\n",
        "            self.GAN_beta1 = 0.5\n",
        "            self.beta1 = 0.9\n",
        "            self.beta2 = 0.999\n",
        "            self.epsilon = 1e-8\n",
        "            self.alpha = 0.5\n",
        "            self.alpha_cla_adv = 0.01\n",
        "            self.init_alpha_p = 0.0 # 0.1, 0.03\n",
        "            self.apply_alpha_p = 0.1\n",
        "            self.apply_epoch = 200 # 200, 300\n",
        "            self.decay_epoch = 50\n",
        "\n",
        "            self.sample_num = 64\n",
        "            self.visual_num = 100\n",
        "            self.len_discrete_code = 10\n",
        "\n",
        "            self.data_X, self.data_y, self.unlabelled_X, self.unlabelled_y, self.test_X, self.test_y = X_train, y_train, X_valid, y_valid, X_test, y_test\n",
        "\n",
        "            self.num_batches = len(self.data_X) // self.batch_size\n",
        "            \n",
        "    def discriminator(self, x, y_, scope='discriminator', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            y = tf.reshape(y_, [-1, 1, 1, self.y_dim])\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], layer_name=scope+'_conv1'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=32, kernel=[3,3], stride=2, layer_name=scope+'_conv2'))\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], layer_name=scope+'_conv3'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=64, kernel=[3,3], stride=2, layer_name=scope+'_conv4'))\n",
        "            x = dropout(x, rate=0.2, is_training=is_training)\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv5'))\n",
        "            x = conv_concat(x,y)\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv6'))\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = Global_Average_Pooling(x)\n",
        "            x = flatten(x)\n",
        "            x = concat([x,y_]) # mlp_concat\n",
        "\n",
        "            x_logit = linear(x, unit=1, layer_name=scope+'_linear1')\n",
        "            out = sigmoid(x_logit)\n",
        "\n",
        "\n",
        "            return out, x_logit, x\n",
        "\n",
        "    def generator(self, z, y, scope='generator', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "\n",
        "            x = concat([z, y]) # mlp_concat\n",
        "\n",
        "            x = relu(linear(x, unit=512*4*4, layer_name=scope+'_linear1'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch1')\n",
        "\n",
        "            x = tf.reshape(x, shape=[-1, 4, 4, 512])\n",
        "            y = tf.reshape(y, [-1, 1, 1, self.y_dim])\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = relu(deconv_layer(x, filter_size=256, kernel=[5,5], stride=2, layer_name=scope+'_deconv1'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch2')\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = relu(deconv_layer(x, filter_size=128, kernel=[5,5], stride=2, layer_name=scope+'_deconv2'))\n",
        "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch3')\n",
        "            x = conv_concat(x,y)\n",
        "\n",
        "            x = tanh(deconv_layer(x, filter_size=3, kernel=[5,5], stride=2, wn=False, layer_name=scope+'deconv3'))\n",
        "\n",
        "            return x\n",
        "    def classifier(self, x, scope='classifier', is_training=True, reuse=False):\n",
        "        with tf.variable_scope(scope, reuse=reuse) :\n",
        "            x = gaussian_noise_layer(x) # default = 0.15\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv1'))\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv2'))\n",
        "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv3'))\n",
        "\n",
        "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
        "            x = dropout(x, rate=0.5, is_training=is_training)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv4'))\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv5'))\n",
        "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv6'))\n",
        "\n",
        "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
        "            x = dropout(x, rate=0.5, is_training=is_training)\n",
        "\n",
        "            x = lrelu(conv_layer(x, filter_size=512, kernel=[3,3], layer_name=scope+'_conv7'))\n",
        "            x = nin(x, unit=256, layer_name=scope+'_nin1')\n",
        "            x = nin(x, unit=128, layer_name=scope+'_nin2')\n",
        "\n",
        "            x = Global_Average_Pooling(x)\n",
        "            x = flatten(x)\n",
        "            x = linear(x, unit=10, layer_name=scope+'_linear1')\n",
        "            return x\n",
        "\n",
        "    def build_model(self):\n",
        "#         image_dims = [self.input_height, self.input_width, self.c_dim]\n",
        "        image_dims = [self.input_height, self.input_width]\n",
        "        bs = self.batch_size\n",
        "        unlabel_bs = self.unlabelled_batch_size\n",
        "        test_bs = self.test_batch_size\n",
        "        alpha = self.alpha\n",
        "        alpha_cla_adv = self.alpha_cla_adv\n",
        "        self.alpha_p = tf.placeholder(tf.float32, name='alpha_p')\n",
        "        self.gan_lr = tf.placeholder(tf.float32, name='gan_lr')\n",
        "        self.cla_lr = tf.placeholder(tf.float32, name='cla_lr')\n",
        "        self.unsup_weight = tf.placeholder(tf.float32, name='unsup_weight')\n",
        "        self.c_beta1 = tf.placeholder(tf.float32, name='c_beta1')\n",
        "\n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        # images\n",
        "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
        "        self.unlabelled_inputs = tf.placeholder(tf.float32, [unlabel_bs] + image_dims, name='unlabelled_images')\n",
        "        self.test_inputs = tf.placeholder(tf.float32, [test_bs] + image_dims, name='test_images')\n",
        "\n",
        "        # labels\n",
        "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
        "        self.unlabelled_inputs_y = tf.placeholder(tf.float32, [unlabel_bs, self.y_dim])\n",
        "        self.test_label = tf.placeholder(tf.float32, [test_bs, self.y_dim], name='test_label')\n",
        "        self.visual_y = tf.placeholder(tf.float32, [self.visual_num, self.y_dim], name='visual_y')\n",
        "\n",
        "        # noises\n",
        "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
        "        self.visual_z = tf.placeholder(tf.float32, [self.visual_num, self.z_dim], name='visual_z')\n",
        "\n",
        "        \"\"\" Loss Function \"\"\"\n",
        "        # A Game with Three Players\n",
        "\n",
        "        # output of D for real images\n",
        "        D_real, D_real_logits, _ = self.discriminator(self.inputs, self.y, is_training=True, reuse=False)\n",
        "\n",
        "        # output of D for fake images\n",
        "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
        "        D_fake, D_fake_logits, _ = self.discriminator(G, self.y, is_training=True, reuse=True)\n",
        "\n",
        "        # output of C for real images\n",
        "        C_real_logits = self.classifier(self.inputs, is_training=True, reuse=False)\n",
        "        R_L = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_real_logits))\n",
        "\n",
        "        # output of D for unlabelled images\n",
        "        Y_c = self.classifier(self.unlabelled_inputs, is_training=True, reuse=True)\n",
        "        D_cla, D_cla_logits, _ = self.discriminator(self.unlabelled_inputs, Y_c, is_training=True, reuse=True)\n",
        "\n",
        "        # output of C for fake images\n",
        "        C_fake_logits = self.classifier(G, is_training=True, reuse=True)\n",
        "        R_P = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_fake_logits))\n",
        "\n",
        "        #\n",
        "\n",
        "        # get loss for discriminator\n",
        "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
        "        d_loss_fake = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
        "        d_loss_cla = alpha*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.zeros_like(D_cla)))\n",
        "        self.d_loss = d_loss_real + d_loss_fake + d_loss_cla\n",
        "\n",
        "        # get loss for generator\n",
        "        self.g_loss = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
        "\n",
        "        # test loss for classify\n",
        "        test_Y = self.classifier(self.test_inputs, is_training=False, reuse=True)\n",
        "        correct_prediction = tf.equal(tf.argmax(test_Y, 1), tf.argmax(self.test_label, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        # get loss for classify\n",
        "        max_c = tf.cast(tf.argmax(Y_c, axis=1), tf.float32)\n",
        "        c_loss_dis = tf.reduce_mean(max_c * tf.nn.softmax_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.ones_like(D_cla)))\n",
        "        # self.c_loss = alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
        "\n",
        "        # R_UL = self.unsup_weight * tf.reduce_mean(tf.squared_difference(Y_c, self.unlabelled_inputs_y))\n",
        "        self.c_loss = alpha_cla_adv * alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "\n",
        "        # divide trainable variables into a group for D and a group for G\n",
        "        t_vars = tf.trainable_variables()\n",
        "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "        c_vars = [var for var in t_vars if 'classifier' in var.name]\n",
        "\n",
        "        for var in t_vars: print(var.name)\n",
        "        # optimizers\n",
        "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "            self.d_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.d_loss, var_list=d_vars)\n",
        "            self.g_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.g_loss, var_list=g_vars)\n",
        "            self.c_optim = tf.train.AdamOptimizer(self.cla_lr, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon).minimize(self.c_loss, var_list=c_vars)\n",
        "\n",
        "        \"\"\"\" Testing \"\"\"\n",
        "        # for test\n",
        "        self.fake_images = self.generator(self.visual_z, self.visual_y, is_training=False, reuse=True)\n",
        "\n",
        "        \"\"\" Summary \"\"\"\n",
        "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
        "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
        "        d_loss_cla_sum = tf.summary.scalar(\"d_loss_cla\", d_loss_cla)\n",
        "\n",
        "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
        "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
        "        c_loss_sum = tf.summary.scalar(\"c_loss\", self.c_loss)\n",
        "\n",
        "\n",
        "\n",
        "        # final summary operations\n",
        "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
        "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
        "        self.c_sum = tf.summary.merge([d_loss_cla_sum, c_loss_sum])\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "        gan_lr = self.learning_rate\n",
        "        cla_lr = self.cla_learning_rate\n",
        "\n",
        "        # graph inputs for visualize training results\n",
        "        self.sample_z = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
        "        self.test_codes = self.data_y[0:self.visual_num]\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
        "            counter = checkpoint_counter\n",
        "            with open('lr_logs.txt', 'r') as f :\n",
        "                line = f.readlines()\n",
        "                line = line[-1]\n",
        "                gan_lr = float(line.split()[0])\n",
        "                cla_lr = float(line.split()[1])\n",
        "                print(\"gan_lr : \", gan_lr)\n",
        "                print(\"cla_lr : \", cla_lr)\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(start_epoch, self.epoch):\n",
        "\n",
        "            if epoch >= self.decay_epoch :\n",
        "                gan_lr *= 0.995\n",
        "                cla_lr *= 0.99\n",
        "                print(\"**** learning rate DECAY ****\")\n",
        "                print(gan_lr)\n",
        "                print(cla_lr)\n",
        "\n",
        "            if epoch >= self.apply_epoch :\n",
        "                alpha_p = self.apply_alpha_p\n",
        "            else :\n",
        "                alpha_p = self.init_alpha_p\n",
        "\n",
        "            rampup_value = rampup(epoch - 1)\n",
        "            unsup_weight = rampup_value * 100.0 if epoch > 1 else 0\n",
        "\n",
        "            # get batch data\n",
        "            for idx in range(start_batch_id, self.num_batches):\n",
        "                batch_images = self.data_X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "                batch_codes = self.data_y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "                batch_unlabelled_images = self.unlabelled_X[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
        "                batch_unlabelled_images_y = self.unlabelled_y[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
        "\n",
        "                batch_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
        "\n",
        "                feed_dict = {\n",
        "                    self.inputs: batch_images, self.y: batch_codes,\n",
        "                    self.unlabelled_inputs: batch_unlabelled_images,\n",
        "                    self.unlabelled_inputs_y: batch_unlabelled_images_y,\n",
        "                    self.z: batch_z, self.alpha_p: alpha_p,\n",
        "                    self.gan_lr: gan_lr, self.cla_lr: cla_lr,\n",
        "                    self.unsup_weight : unsup_weight\n",
        "                }\n",
        "                # update D network\n",
        "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss], feed_dict=feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # update G network\n",
        "                _, summary_str_g, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict=feed_dict)\n",
        "                self.writer.add_summary(summary_str_g, counter)\n",
        "\n",
        "                # update C network\n",
        "                _, summary_str_c, c_loss = self.sess.run([self.c_optim, self.c_sum, self.c_loss], feed_dict=feed_dict)\n",
        "                self.writer.add_summary(summary_str_c, counter)\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, c_loss: %.8f\" \\\n",
        "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, c_loss))\n",
        "\n",
        "                # save training results for every 100 steps\n",
        "                \"\"\"\n",
        "                if np.mod(counter, 100) == 0:\n",
        "                    samples = self.sess.run(self.fake_images,\n",
        "                                            feed_dict={self.z: self.sample_z, self.y: self.test_codes})\n",
        "                    image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
        "                    save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
        "                                './' + check_folder(\n",
        "                                    self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
        "                                    epoch, idx))\n",
        "                \"\"\"\n",
        "\n",
        "            # classifier test\n",
        "            test_acc = 0.0\n",
        "\n",
        "            for idx in range(10) :\n",
        "                test_batch_x = self.test_X[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
        "                test_batch_y = self.test_y[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
        "\n",
        "                acc_ = self.sess.run(self.accuracy, feed_dict={\n",
        "                    self.test_inputs: test_batch_x,\n",
        "                    self.test_label: test_batch_y\n",
        "                })\n",
        "\n",
        "                test_acc += acc_\n",
        "            test_acc /= 10\n",
        "\n",
        "            summary_test = tf.Summary(value=[tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
        "            self.writer.add_summary(summary_test, epoch)\n",
        "\n",
        "            line = \"Epoch: [%2d], test_acc: %.4f\\n\" % (epoch, test_acc)\n",
        "            print(line)\n",
        "            lr = \"{} {}\".format(gan_lr, cla_lr)\n",
        "            with open('logs.txt', 'a') as f:\n",
        "                f.write(line)\n",
        "            with open('lr_logs.txt', 'a') as f :\n",
        "                f.write(lr+'\\n')\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # show temporal results\n",
        "            self.visualize_results(epoch)\n",
        "\n",
        "            # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    def visualize_results(self, epoch):\n",
        "        # tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "        image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
        "        z_sample = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
        "\n",
        "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
        "        y = np.random.choice(self.len_discrete_code, self.visual_num)\n",
        "        # Generated 10 labels with batch_size\n",
        "        y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
        "        y_one_hot[np.arange(self.visual_num), y] = 1\n",
        "\n",
        "        samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
        "\n",
        "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
        "                    check_folder(\n",
        "                        self.result_dir + '/' + self.model_dir + '/all_classes') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
        "\n",
        "        \"\"\" specified condition, random noise \"\"\"\n",
        "        n_styles = 10  # must be less than or equal to self.batch_size\n",
        "\n",
        "        np.random.seed()\n",
        "        si = np.random.choice(self.visual_num, n_styles)\n",
        "\n",
        "        for l in range(self.len_discrete_code):\n",
        "            y = np.zeros(self.visual_num, dtype=np.int64) + l\n",
        "            y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
        "            y_one_hot[np.arange(self.visual_num), y] = 1\n",
        "\n",
        "            samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
        "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
        "                        check_folder(\n",
        "                            self.result_dir + '/' + self.model_dir + '/class_%d' % l) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
        "\n",
        "            samples = samples[si, :, :, :]\n",
        "\n",
        "            if l == 0:\n",
        "                all_samples = samples\n",
        "            else:\n",
        "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
        "\n",
        "        \"\"\" save merged images to check style-consistency \"\"\"\n",
        "        canvas = np.zeros_like(all_samples)\n",
        "        for s in range(n_styles):\n",
        "            for c in range(self.len_discrete_code):\n",
        "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
        "\n",
        "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
        "                    check_folder(\n",
        "                        self.result_dir + '/' + self.model_dir + '/all_classes_style_by_style') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return \"{}_{}_{}_{}\".format(\n",
        "            self.model_name, self.dataset_name,\n",
        "            self.batch_size, self.z_dim)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        import re\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BN3QzRJTVB-U"
      },
      "cell_type": "markdown",
      "source": [
        "#### evaluate model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lMZWcfKFTUkZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "    gan = TripleGAN(sess, epoch=args.epoch, batch_size=args.batch_size, unlabel_batch_size=args.unlabel_batch_size,\n",
        "                        z_dim=args.z_dim, dataset_name=args.dataset, n=args.n, gan_lr = args.gan_lr, cla_lr = args.cla_lr,\n",
        "                        checkpoint_dir=args.checkpoint_dir, result_dir=args.result_dir, log_dir=args.log_dir)\n",
        "\n",
        "    # build graph\n",
        "    gan.build_model()\n",
        "\n",
        "    # show network architecture\n",
        "    show_all_variables()\n",
        "\n",
        "    # launch the graph in a session\n",
        "    gan.train()\n",
        "    print(\" [*] Training finished!\")\n",
        "\n",
        "    # visualize learned generator\n",
        "    gan.visualize_results(args.epoch-1)\n",
        "    print(\" [*] Testing finished!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d4zFm5T0VB-Y"
      },
      "cell_type": "markdown",
      "source": [
        "#### submit prediction"
      ]
    },
    {
      "metadata": {
        "id": "RqPn3GLXUpMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}