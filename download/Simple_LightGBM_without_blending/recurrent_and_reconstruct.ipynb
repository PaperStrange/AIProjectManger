{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_and_reconstruct.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "iOenr4RTZxsM",
        "lZuxvfnMZ6Q3",
        "jWPEV4jeaCTu",
        "X0MBhai6aTQO",
        "zLFj094saYLa",
        "Fzf851BEaeCe"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "yAjXGPfiGxc7"
      },
      "cell_type": "markdown",
      "source": [
        "* Author: HZQ\n",
        "* Last modified: 2018/2/24\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vV5qe6BlG1_C",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools \n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null \n",
        "!apt-get update -qq 2>&1 > /dev/null \n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse \n",
        "from google.colab import auth \n",
        "auth.authenticate_user() \n",
        "from oauth2client.client import GoogleCredentials \n",
        "creds = GoogleCredentials.get_application_default() \n",
        "import getpass \n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL \n",
        "vcode = getpass.getpass() \n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "T0HVL61FHqMC",
        "outputId": "3624bdca-12f8-45cd-a3ce-e38849854529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p driver\n",
        "!google-drive-ocamlfuse driver\n",
        "import os\n",
        "\n",
        "os.chdir(\"driver/Colab Notebooks/CategoryRecom_kaggle/\")\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            "data\t  driver   model      requirements.txt\ttimeline\n",
            "download  LICENSE  ReadMe.md  submit\t\tutils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8OwoC4PMaAbC",
        "outputId": "dd2119f3-d6f7-4b44-9a9d-746232c707d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "driver\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6EglTV4uGxdA"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Parts"
      ]
    },
    {
      "metadata": {
        "id": "iOenr4RTZxsM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### change debug mode and Import package"
      ]
    },
    {
      "metadata": {
        "id": "M0wVXTxSzI9X",
        "colab_type": "code",
        "outputId": "84bfeadc-3f2c-4288-ea62-f1eaf343ca9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# Used for better debug experience\n",
        "%xmode Plain\n",
        "# %pdb on"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Plain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qG85KhyNGxdK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import gc, sys\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
        "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
        "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
        "                  'OOF_PRED', 'month_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lZuxvfnMZ6Q3"
      },
      "cell_type": "markdown",
      "source": [
        "### allocate data path"
      ]
    },
    {
      "metadata": {
        "id": "dGXHmH-TFx0u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_ROOT_PATH = \"./data/\"\n",
        "\n",
        "data_train_path = DATA_ROOT_PATH + \"data_raw/train/train.csv\"\n",
        "# data_train_path = DATA_ROOT_PATH + \"data_raw/train/train_20190218.csv\"\n",
        "data_test_path = DATA_ROOT_PATH + \"data_raw/test/test.csv\"\n",
        "data_valid_path = DATA_ROOT_PATH + \"data_raw/valid/valid.csv\"\n",
        "data_info_path = DATA_ROOT_PATH  + \"data_raw/Data_Dictionary.xlsx\"\n",
        "data_merchants_path = DATA_ROOT_PATH + \"data_raw/merchants.csv\"\n",
        "# data_merchants_path = DATA_ROOT_PATH + \"data_raw/merchants_20190219.csv\"\n",
        "data_his_trans_path = DATA_ROOT_PATH + \"data_raw/historical_transactions_20190218.csv\"\n",
        "data_new_trans_path = DATA_ROOT_PATH + \"data_raw/new_merchant_transactions.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jWPEV4jeaCTu"
      },
      "cell_type": "markdown",
      "source": [
        "### define fundamental-use functions"
      ]
    },
    {
      "metadata": {
        "id": "HENkr0SjG0Ff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# month_dict = \\\n",
        "#         {1:fundemental\"January\", \n",
        "#          2:\"February\", \n",
        "#          3:\"March\", \n",
        "#          4:\"April\", \n",
        "#          5:\"May\", \n",
        "#          6:\"June\", \n",
        "#          7:\"July\", \n",
        "#          8:\"August\", \n",
        "#          9:\"September\", \n",
        "#          10:\"October\", \n",
        "#          11:\"November\", \n",
        "#          12:\"December\"}\n",
        "\n",
        "@contextmanager\n",
        "def timer(title):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
        "\n",
        "    \n",
        "# rmse\n",
        "def rmse(y_true, y_pred):\n",
        "    \n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "# One-hot encoding for categorical columns with get_dummies\n",
        "def one_hot_encoder(df, nan_as_category = True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    \n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    return df, new_columns\n",
        "\n",
        "\n",
        "# reduce memory\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     print('Memory fundementalusage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "X0MBhai6aTQO"
      },
      "cell_type": "markdown",
      "source": [
        "### define re-construct functions"
      ]
    },
    {
      "metadata": {
        "id": "gQDQSO0BDFTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Re-construct 2019/2/17: fill vacancy merchant id by other method [here](https://www.kaggle.com/raddar/merchant-id-imputations)\n",
        "# def merchant_id_selected():\n",
        "#     new_merchant_df = pd.read_csv(data_new_trans_path).fillna('')\n",
        "#     fields = ['card_id','city_id','category_1','installments','category_3',\\\n",
        "#           'merchant_category_id','category_2','state_id','subsector_id']\n",
        "#     new_merchant_df_part = new_merchant_df[fields + ['merchant_id']].drop_duplicates()\n",
        "#     new_merchant_df_part = new_merchant_df_part.loc[new_merchant_df_part['merchant_id'] != '']  \n",
        "#     # take only unique merchants for the `fields` combination\n",
        "#     uq_new_merchants = new_merchant_df_part.groupby(fields)['merchant_id'].count().reset_index(name = 'n_merchants')\n",
        "#     uq_new_merchants = uq_new_merchants.loc[uq_new_merchants['n_merchants'] == 1]\n",
        "#     uq_new_merchants = uq_new_merchants.merge(new_merchant_df_part, on=fields)\n",
        "#     uq_new_merchants.drop('n_merchants', axis=1, inplace=True)\n",
        "#     # rename the merchant_id so we can join it more easily later on\n",
        "#     uq_new_merchants.columns = fields + ['imputed_merchant_id']\n",
        "    \n",
        "#     del new_merchant_df\n",
        "#     del new_merchant_df_part\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()    \n",
        "    \n",
        "#     return uq_new_merchants, fields\n",
        "\n",
        "\n",
        "# Re-construct 2019/2/17: fill vacancy merchant id by other method [here](https://www.kaggle.com/raddar/merchant-id-imputations)\n",
        "# def impute_his_merchant_id():\n",
        "#     hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)\n",
        "#     hist_list = []\n",
        "#     indent = 1    \n",
        "\n",
        "#     for each_chunk in hist_reader:\n",
        "#         print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "#         each_chunk = reduce_mem_usage(each_chunk)\n",
        "#         hist_list.append(each_chunk)\n",
        "#         indent += 1\n",
        "#         gc.collect()\n",
        "#     sys._clear_type_cache()        \n",
        "    \n",
        "#     hist_df = pd.concat(hist_list, ignore_index=True)\n",
        "#     del hist_list\n",
        "#     del each_chunk\n",
        "#     del hist_reader\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()  \n",
        "    \n",
        "#     # make the actual imputation for the merchant_id field   \n",
        "#     uq_new_merchants, fields = merchant_id_selected()  \n",
        "#     hist_df.fillna('', inplace=True) \n",
        "#     hist_df = hist_df.merge(uq_new_merchants, on = fields, how = 'left')\n",
        "#     index = (hist_df['merchant_id']=='') & (~pd.isnull(hist_df['imputed_merchant_id']))\n",
        "#     hist_df.loc[index, 'merchant_id'] = hist_df.loc[index, 'imputed_merchant_id']\n",
        "#     hist_df.drop([\"imputed_merchant_id\"], axis=1, inplace=True)\n",
        "#     hist_df.to_csv(DATA_ROOT_PATH + \"data_raw/historical_transactions_20190219.csv\", index=False)\n",
        "    \n",
        "#     return hist_df\n",
        "\n",
        "\n",
        "# hist_df = impute_his_merchant_id()\n",
        "\n",
        "\n",
        "# Re-construct 2019/2/17: use new retrieved target values\n",
        "# data_train = pd.read_csv(data_train_path)\n",
        "# data_train[\"target\"] = np.exp2(data_train[\"target\"]) - 0.0000000001\n",
        "# data_train_path = DATA_ROOT_PATH + \"data_raw/train/train_20190219.csv\"\n",
        "# data_train.to_csv(data_train_path)\n",
        "\n",
        "\n",
        "# # Re-construct 2019/2/19: design aggregation for merchant.csv\n",
        "# def aggregate_merchants(df, df_merchant):\n",
        "#     '''\n",
        "#     INPUT:\n",
        "#     df -  a pandas dataframe containing data after dealing with outliers, mixed feature\n",
        "#     df_merchant -  a specific pandas dataframe containing data of several merchants\n",
        "    \n",
        "#     OUTPUT:\n",
        "#     df_agg_merchant -  a pandas dataframe after aggregation from merchants csv file\n",
        "    \n",
        "#     This function applly specific aggregation to INPUT.df\n",
        "#     '''\n",
        "\n",
        "#     df_agg_merchant = pd.merge(df, df_merchant, on=\"merchant_id\", how=\"left\")    \n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()   \n",
        "    \n",
        "#     return df_agg_merchant\n",
        "\n",
        "\n",
        "# data_merchants= pd.read_csv(data_merchants_path)\n",
        "# data_merchants.drop([\"merchant_category_id\", \"city_id\", \"merchant_group_id\"], axis=1, inplace=True)\n",
        "# for each_col in ['category_1', 'category_4']:\n",
        "#     data_merchants[each_col] = data_merchants[each_col].map({\"Y\":1, \"N\":0})\n",
        "# data_merchants = pd.get_dummies(data_merchants, columns=[\"most_recent_sales_range\", \"most_recent_purchases_range\"])\n",
        "    \n",
        "# print(\"Excecute aggregation to VARIABLE data_merchants\")\n",
        "# print(\"[Input] Aggregate specific columns below\")\n",
        "# agg_func = {\n",
        "#         \"active_months_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"active_months_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"active_months_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"category_1\": [\"sum\", \"mean\"],\n",
        "#         \"category_2\": [\"sum\", \"mean\"],\n",
        "#         \"category_4\": [\"sum\", \"mean\"],\n",
        "#         \"most_recent_purchases_range_A\": [\"mean\"],        \n",
        "#         \"most_recent_purchases_range_B\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_C\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_D\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_E\": [\"mean\"],    \n",
        "#         \"most_recent_sales_range_A\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_B\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_C\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_D\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_E\": [\"mean\"], \n",
        "#         \"numerical_1\": [\"sum\", \"mean\"],\n",
        "#         \"numerical_2\": [\"sum\", \"mean\"]\n",
        "#  }\n",
        "\n",
        "# data_merchants_agg = data_merchants.groupby([\"merchant_id\"]).agg(agg_func)\n",
        "# data_merchants_agg.reset_index(inplace=True)\n",
        "# data_merchants_agg.columns = [\"_\".join(col).strip() for col in data_merchants_agg.columns.values]\n",
        "# del data_merchants\n",
        "# gc.collect()\n",
        "# sys._clear_type_cache()\n",
        "\n",
        "# print(\"[Output] Store aggregated merchant data file: \")\n",
        "# temp = list(data_merchants_agg.columns)\n",
        "# temp[0] = \"merchant_id\"\n",
        "# data_merchants_agg.columns = temp\n",
        "# data_merchants_agg.to_csv( DATA_ROOT_PATH + \"data_raw/merchants_20190219.csv\", index=False)\n",
        "\n",
        "# Re-construct 2019/2/20: ADDITIONAL MONTHLY PURCHASE AMOUNT RATIO FEATURE  \n",
        "def groupby_row1(each_row1):\n",
        "    each_row1_part = each_row1.loc[:, [\"merchant_id\", \"month_lag\", \"purchase_amount\"]]\n",
        "    del each_row1\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    unique_merchant_ids = each_row1_part[\"merchant_id\"].drop_duplicates()\n",
        "    ratio_per_merchant_id = []\n",
        "    \n",
        "    for each_merchant in unique_merchant_ids:\n",
        "        temp = each_row1_part[each_row1_part[\"merchant_id\"] == each_merchant]\n",
        "        temp = temp.groupby(\"month_lag\").mean()\n",
        "#         print(\"After grouping: {}\".format(temp))\n",
        "        temp.reset_index(inplace=True)\n",
        "#         print(\"After re-indexing: {}\".format(temp))\n",
        "        gc.collect()\n",
        "        sys._clear_type_cache()\n",
        "        if temp.shape[0] == 1 or temp.shape[0] == 0:\n",
        "            ratio_per_merchant_id.append(0)\n",
        "        else:\n",
        "            print(\"Bingo!\")\n",
        "            ratio_per_merchant_id.append(\n",
        "                np.mean([\n",
        "                    np.power(\n",
        "                    each_row1_part.loc[i+1, \"purchase_amount\"] / each_row1_part.loc[i, \"purchase_amount\"], \n",
        "                    1.0 / (each_row1_part.loc[i+1, \"month_lag\"] - each_row1_part.loc[i, \"month_lag\"])\n",
        "                ) for i in range(each_row1_part.shape[0]-1)\n",
        "                            ])\n",
        "             )\n",
        "    \n",
        "    ratio_per_card_id = np.mean(ratio_per_merchant_id)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()    \n",
        "    print(\"Check result: {}\".format(ratio_per_card_id))\n",
        "        \n",
        "    return ratio_per_card_id          \n",
        "\n",
        "\n",
        "def purchase_amount_ratio(df):\n",
        "#     from collections import defaultdict\n",
        "#     unique_card_ids = df['card_id'].drop_duplicates()\n",
        "#     n = len(unique_card_ids)\n",
        "#     feature_df = pd.DataFrame({'card_id': unique_card_ids,\n",
        "#                                'monthly_purchase_amount_ratio': np.zeros(len(unique_card_ids))})\n",
        "\n",
        "#     for j, card_id in enumerate(unique_card_ids):\n",
        "#         card_df = df.loc[df['card_id'] == card_id, :]\n",
        "#         merchant_ids = df.loc[df['card_id'] == card_id, 'merchant_id']\n",
        "#         ratio = []\n",
        "#         print(len(merchant_ids))\n",
        "#         for merchant in merchant_ids:\n",
        "#             purchase_amount = card_df.loc[card_df['merchant_id'] == merchant, 'purchase_amount']\n",
        "#             month_lag = card_df.loc[card_df['merchant_id'] == merchant, 'month_lag']\n",
        "#             if len(purchase_amount):\n",
        "#                 unique_month_lag = month_lag.drop_duplicates()\n",
        "#                 pa_dict = defaultdict(float)\n",
        "#                 for i in range(len(purchase_amount)):\n",
        "#                     pa_dict[month_lag.values[i]] += purchase_amount.values[i]\n",
        "#                 m_ratio = []\n",
        "#                 s_lag = np.sort(unique_month_lag)\n",
        "#                 for i in range(len(unique_month_lag) - 1):\n",
        "#                     m_ratio.append(np.power(pa_dict[s_lag[i+1]] / pa_dict[s_lag[i]], 1.0 / (s_lag[i+1] - s_lag[i])))\n",
        "#                 if len(m_ratio):\n",
        "#                     ratio.append(np.mean(m_ratio))\n",
        "#         if len(ratio):\n",
        "#             feature_df.loc[feature_df['card_id'] == card_id, 'monthly_purchase_amount_ratio'] = np.mean(ratio)\n",
        "#         if (j+1) % 100 == 0 or j+1 == n:\n",
        "#             print('processed {}/{} unique card ids.'.format(j+1, n))\n",
        "\n",
        "    feature_df_new = df.groupby(\"card_id\").apply(groupby_row1)\n",
        "    assert df.groupby(\"card_id\").mean(),shape[0] == feature_df_new.shape[0]\n",
        "#     pd.MultiIndex.from_frame(df, names=None)\n",
        "    del df\n",
        "    \n",
        "    return feature_df_new\n",
        "\n",
        "# Re-construct 2019/2/21: create transaction couts, state/city counts for each card id in two transaction files \n",
        "def groupby_row2(each_row1):\n",
        "#     each_row1_part = each_row1.loc[:, [\"city_id\", \"merchant_category_id\", \"merchant_id\"]]\n",
        "    each_row1_part = each_row1.loc[:, [\"state_id\", \"city_id\", \"merchant_category_id\", \"merchant_id\"]]\n",
        "    del each_row1\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    trans_count = each_row1_part.shape[0]\n",
        "    state_count = each_row1_part[\"state_id\"].value_counts().shape[0]\n",
        "    city_count = each_row1_part[\"city_id\"].value_counts().shape[0]\n",
        "    merchant_category_count = each_row1_part[\"merchant_category_id\"].value_counts().shape[0]\n",
        "    merchant_count = each_row1_part[\"merchant_id\"].value_counts().shape[0]\n",
        "#     print(each_row1_part)\n",
        "#     print(\"Checking transacation count: {}\".format(trans_count))\n",
        "    \n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()    \n",
        "        \n",
        "    return city_count, merchant_category_count, merchant_count      \n",
        "\n",
        "\n",
        "def count_by_card_id(df):\n",
        "    feature_new_df = df.groupby(\"card_id\").apply(groupby_row2)\n",
        "    del df\n",
        "    \n",
        "    return feature_new_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vsdcNW0Zdalf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test = pd.read_csv(data_new_trans_path)\n",
        "# feature_df = purchase_amount_ratio(test)\n",
        "# purchase_amount_ratio(test)\n",
        "\n",
        "\n",
        "hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)   # Note: modify the chunksize if crash\n",
        "feats_new_list = []\n",
        "indent = 1\n",
        "for each_chunk in hist_reader:\n",
        "    print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "    each_chunk = reduce_mem_usage(each_chunk)      \n",
        "    feats_new_series = count_by_card_id(each_chunk)\n",
        "    print(\"Checking each output: {}\".format(feats_new_series))\n",
        "    feats_new_list.append(feats_new_series)\n",
        "    indent += 1\n",
        "    gc.collect()\n",
        "sys._clear_type_cache()    \n",
        "\n",
        "del each_chunk\n",
        "del hist_reader\n",
        "feats_new_series = pd.concat(feats_new_list, ignore_index=True)\n",
        "\n",
        "gc.collect()\n",
        "sys._clear_type_cache()\n",
        "\n",
        "feats_new_series.to_csv(DATA_ROOT_PATH + \"data_raw/feats_new_20190223.csv\", index=False)\n",
        "# feats_new_series = pd.read_csv(DATA_ROOT_PATH + \"data_raw/feats_new_20190223.csv\", header=0)\n",
        "\n",
        "print(\"Total number of part is {}\".format(len(feats_new_list)))\n",
        "index_total = feats_new_list[0].index.append(feats_new_list[1].index).append(feats_new_list[2].index)\n",
        "index_series = pd.Series(index_total)\n",
        "index_series.to_csv(DATA_ROOT_PATH + \"data_raw/his_trans_index_after_agg.csv\", index=False)\n",
        "feats_new_df = pd.DataFrame({\"card_id\": index_series.values})\n",
        "\n",
        "ind = 0\n",
        "feats_new_list, feats_new_list2 = \\\n",
        "    [each_row for each_row in feats_new_series.values], []\n",
        "\n",
        "for each_feat in zip(*feats_new_list):\n",
        "#     print(each_feat)\n",
        "    # if ind == 2:\n",
        "        # break\n",
        "    if not(\"(\" in each_feat) and not(\")\" in each_feat) and not(\",\" in each_feat):    \n",
        "        feats_new_list2.append(each_feat)\n",
        "    ind += 1\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "assert len(feats_new_list2) == 5\n",
        "\n",
        "ind = 0\n",
        "feats_new = \\\n",
        "    [\"his_trans_count\", \n",
        "     \"his_state_id_count\", \n",
        "     \"his_city_id_count\", \n",
        "     \"his_merchant_category_id_count\", \n",
        "     \"his_merchant_id_count\"]\n",
        "for each_feat_new in feats_new[:]:\n",
        "    feats_new_df[each_feat_new] = feats_new_list2[ind]\n",
        "    print(\"Finish {} feat dealing process...\".format(ind+1))\n",
        "    ind += 1\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "feats_new_df[\"his_trans_count\"] = \\\n",
        "    feats_new_df[\"his_trans_count\"].astype(\"int\")\n",
        "feats_new_df[\"his_state_id_count\"] = \\\n",
        "    feats_new_df[\"his_state_id_count\"].astype(\"int\")\n",
        "feats_new_df[\"his_city_id_count\"] = \\\n",
        "    feats_new_df[\"his_city_id_count\"].astype(\"int\")\n",
        "feats_new_df[\"his_merchant_category_id_count\"] = \\\n",
        "    feats_new_df[\"his_merchant_category_id_count\"].astype(\"int\")\n",
        "feats_new_df[\"his_merchant_id_count\"] = \\\n",
        "    feats_new_df[\"his_merchant_id_count\"].astype(\"int\")\n",
        "feats_new_df.to_csv(DATA_ROOT_PATH + \"data_raw/feats_new_20190223_2.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zLFj094saYLa"
      },
      "cell_type": "markdown",
      "source": [
        "### define data-processing functions"
      ]
    },
    {
      "metadata": {
        "id": "gB3M2g3MESwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 2019/2/18: MIDfc7d7969c3 = Netflix subscription prices\n",
        "\n",
        "\n",
        "# preprocessing train & test\n",
        "def train_test(num_rows=None):\n",
        "\n",
        "    # load csv\n",
        "    train_df = pd.read_csv(data_train_path, index_col=['card_id'], nrows=num_rows)\n",
        "    test_df = pd.read_csv(data_test_path, index_col=['card_id'], nrows=num_rows)\n",
        "\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
        "\n",
        "    # outlier\n",
        "    train_df['outliers'] = 0\n",
        "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
        "    \n",
        "    # Re-construct: drop outliers\n",
        "#     index_outliers = (train_df.loc[train_df[\"outliers\"] == 1, :]).index\n",
        "#     train_df.drop(index_outliers, axis=0, inplace=True)\n",
        "#     print(index_outliers)\n",
        "\n",
        "    # set target as nan\n",
        "    test_df['target'] = np.nan\n",
        "\n",
        "    # merge\n",
        "    df = train_df.append(test_df)\n",
        "\n",
        "    del train_df, test_df\n",
        "    gc.collect()\n",
        "\n",
        "    # to datetime\n",
        "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
        "\n",
        "    # datetime features\n",
        "    df['quarter'] = df['first_active_month'].dt.quarter\n",
        "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
        "    \n",
        "    # Re-construct: add new elasp startoff\n",
        "#     df[\"elasped_time_sp\"] = (datetime.date(2018, 2, 1) - df['first_active_month']).dt.days\n",
        "    \n",
        "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
        "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
        "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
        "\n",
        "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
        "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
        "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
        "\n",
        "    # one hot encoding\n",
        "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
        "\n",
        "    for f in ['feature_1','feature_2','feature_3']:\n",
        "        order_label = df.groupby([f])['outliers'].mean()\n",
        "        df[f] = df[f].map(order_label)\n",
        "\n",
        "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
        "    df['feature_mean'] = df['feature_sum'] / 3\n",
        "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
        "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
        "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
        "    \n",
        "    df = reduce_mem_usage(df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# preprocessing historical transactions\n",
        "def historical_transactions(num_rows=None):\n",
        "    # load csv\n",
        "    hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)\n",
        "    hist_list = []\n",
        "    indent = 1\n",
        "    for each_chunk in hist_reader:\n",
        "        print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "        each_chunk = reduce_mem_usage(each_chunk)      \n",
        "        hist_list.append(each_chunk)\n",
        "        indent += 1\n",
        "        gc.collect()\n",
        "    sys._clear_type_cache()        \n",
        "    \n",
        "    hist_df = pd.concat(hist_list, ignore_index=True)\n",
        "    del hist_list\n",
        "    del each_chunk\n",
        "    del hist_reader\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()   \n",
        "    \n",
        "    # fillna    \n",
        "#     hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)     \n",
        "    hist_df['category_2'].fillna(1.0, inplace=True)\n",
        "    hist_df['category_3'].fillna('A', inplace=True)\n",
        "    hist_df['installments'].replace(-1, np.nan, inplace=True)\n",
        "    hist_df['installments'].replace(999, np.nan, inplace=True)\n",
        "  \n",
        "    # Re-construct 2019/2/17: create new \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
        "    hist_df[\"purchase_amount\"] = np.round(hist_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
        "    \n",
        "    # trim\n",
        "#     hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "    # Y/N to 1/0\n",
        "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
        "\n",
        "    # datetime features\n",
        "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
        "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
        "    \n",
        "    # Re-construct 2019/2/18: create categorical data for each month     \n",
        "#     hist_df[\"month_session\"] = hist_df[\"month\"].map(month_dict)\n",
        "#     hist_df = pd.get_dummies(hist_df, columns=[\"month_session\"], drop_first=False)\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()\n",
        "    # Or:\n",
        "#     month_session_cols_list = [\"month_session\"+str(ind+1) for ind in range(12)]\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "#     temp = pd.DataFrame(data=le.fit_transform(hist_df[\"month_session\"]), columns=month_session_cols_list)\n",
        "#     hist_df = pd.concat([hist_df, temp], ignore_index=True)\n",
        "#     print(hist_df.isnull())\n",
        "    \n",
        "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
        "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
        "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
        "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
        "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
        "\n",
        "    # additional features\n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
        "    \n",
        "    #Christmas : December 25 2017\n",
        "    hist_df['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Mothers Day: May 14 2017\n",
        "    hist_df['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #fathers day: August 13 2017\n",
        "    hist_df['fathers_day_2017'] = (pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Childrens day: October 12 2017\n",
        "    hist_df['Children_day_2017'] = (pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Valentine's Day : 12th June, 2017\n",
        "    hist_df['Valentine_Day_2017'] = (pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Black Friday : 24th November 2017\n",
        "    hist_df['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    #2018\n",
        "    #Mothers Day: May 13 2018\n",
        "    hist_df['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
        "    hist_df['month_diff'] += hist_df['month_lag']\n",
        "\n",
        "    # additional features \n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    hist_df['duration'] = hist_df['purchase_amount'] * hist_df['month_diff']\n",
        "    hist_df['amount_month_ratio'] = hist_df['purchase_amount'] / hist_df['month_diff']\n",
        "    \n",
        "    # reduce memory usage\n",
        "    hist_df = reduce_mem_usage(hist_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
        "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
        "\n",
        "    aggs = {}\n",
        "    for col in col_unique:\n",
        "        aggs[col] = ['nunique']\n",
        "\n",
        "    for col in col_seas:\n",
        "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
        "\n",
        "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
        "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
        "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
        "    aggs['purchase_date'] = ['max','min']\n",
        "    \n",
        "    # Re-construct 2019/2/19: add month aggregation     \n",
        "    aggs['month'] = ['max','min','mean','var','skew']\n",
        "#     aggs[\"month_session_April\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_August\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_December\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_February\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_January\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_July\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_June\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_March\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_May\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_November\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_October\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_September\"] = [\"mean\"]\n",
        "    \n",
        "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
        "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
        "    aggs['authorized_flag'] = ['mean']\n",
        "    aggs['weekend'] = ['mean'] # overwrite\n",
        "    aggs['weekday'] = ['mean'] # overwrite\n",
        "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
        "    aggs['category_1'] = ['mean']\n",
        "    aggs['category_2'] = ['mean']\n",
        "    aggs['category_3'] = ['mean']\n",
        "    aggs['card_id'] = ['size','count']\n",
        "    aggs['price'] = ['sum','mean','max','min','var']\n",
        "#     aggs['price_new'] = ['sum','mean','max','min','var']    \n",
        "    aggs['Christmas_Day_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2017'] = ['mean']\n",
        "    aggs['fathers_day_2017'] = ['mean']\n",
        "    aggs['Children_day_2017'] = ['mean']\n",
        "    aggs['Valentine_Day_2017'] = ['mean']\n",
        "    aggs['Black_Friday_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2018'] = ['mean']\n",
        "    aggs['duration']=['mean','min','max','var','skew']\n",
        "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
        "#     aggs['duration_new']=['mean','min','max','var','skew']\n",
        "#     aggs['amount_month_ratio_new']=['mean','min','max','var','skew']\n",
        "\n",
        "    for col in ['category_2','category_3']:\n",
        "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
        "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
        "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
        "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
        "        aggs[col+'_mean'] = ['mean']\n",
        "\n",
        "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    # change column name\n",
        "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
        "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
        "\n",
        "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max'] - hist_df['hist_purchase_date_min']).dt.days\n",
        "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff'] / hist_df['hist_card_id_size']\n",
        "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
        "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
        "\n",
        "    # reduce memory usage\n",
        "    hist_df = reduce_mem_usage(hist_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    hist_df.reset_index(inplace=True)\n",
        "    \n",
        "    return hist_df\n",
        " \n",
        "    \n",
        "# preprocessing new_merchant_transactions\n",
        "def new_merchant_transactions(num_rows=None):\n",
        "    # load csv\n",
        "    new_merchant_df = pd.read_csv(data_new_trans_path, nrows=num_rows)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # fillna\n",
        "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
        "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
        "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
        "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
        "    \n",
        "    # Re-construct 2019/2/17: create new \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
        "    new_merchant_df[\"purchase_amount\"] = np.round(new_merchant_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
        "    \n",
        "    # trim\n",
        "#     new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "    # Y/N to 1/0\n",
        "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
        "\n",
        "    # datetime features\n",
        "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
        "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
        "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
        "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
        "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
        "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
        "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
        "\n",
        "    # Re-construct 2019/2/18: create categorical data for each month     \n",
        "#     new_merchant_df[\"month_session\"] = new_merchant_df[\"month\"].map(month_dict)\n",
        "#     new_merchant_df = pd.get_dummies(new_merchant_df, columns=[\"month_session\"], drop_first=False)\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()\n",
        "    # Or:\n",
        "#     month_session_cols_list = [\"month_session\"+str(ind+1) for ind in range(12)]\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "#     temp = pd.DataFrame(data=le.fit_transform(new_merchant_df[\"month_session\"]), columns=month_session_cols_list)\n",
        "#     new_merchant_df = pd.concat([new_merchant_df, temp], ignore_index=True)\n",
        "#     print(new_merchant_df.isnull())\n",
        "    \n",
        "    # additional features \n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']     \n",
        "    \n",
        "    #Christmas : December 25 2017\n",
        "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Childrens day: October 12 2017\n",
        "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Black Friday : 24th November 2017\n",
        "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    #Mothers Day: May 13 2018\n",
        "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
        "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
        "\n",
        "    # additional features\n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    new_merchant_df['duration'] = new_merchant_df['purchase_amount'] * new_merchant_df['month_diff']\n",
        "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount'] / new_merchant_df['month_diff']\n",
        "    \n",
        "    # reduce memory usage\n",
        "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
        "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
        "\n",
        "    aggs = {}\n",
        "    for col in col_unique:\n",
        "        aggs[col] = ['nunique']\n",
        "\n",
        "    for col in col_seas:\n",
        "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
        "\n",
        "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
        "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
        "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
        "    aggs['purchase_date'] = ['max','min']\n",
        "    \n",
        "    # Re-construct 2019/2/19: add month aggregation     \n",
        "    aggs['month'] = ['max','min','mean','var','skew']\n",
        "#     aggs[\"month_session_April\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_August\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_December\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_February\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_January\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_July\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_June\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_March\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_May\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_November\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_October\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_September\"] = [\"mean\"]\n",
        "        \n",
        "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
        "    aggs['month_diff'] = ['mean','var','skew']\n",
        "    aggs['weekend'] = ['mean']\n",
        "    aggs['month'] = ['mean', 'min', 'max']\n",
        "    aggs['weekday'] = ['mean', 'min', 'max']\n",
        "    aggs['category_1'] = ['mean']\n",
        "    aggs['category_2'] = ['mean']\n",
        "    aggs['category_3'] = ['mean']\n",
        "    aggs['card_id'] = ['size','count']\n",
        "    aggs['price'] = ['mean','max','min','var']\n",
        "#     aggs['price_new'] = ['sum','mean','max','min','var']\n",
        "    aggs['Christmas_Day_2017'] = ['mean']\n",
        "    aggs['Children_day_2017'] = ['mean']\n",
        "    aggs['Black_Friday_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2018'] = ['mean']\n",
        "    aggs['duration']=['mean','min','max','var','skew']\n",
        "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
        "#     aggs['duration_new']=['mean','min','max','var','skew']\n",
        "#     aggs['amount_month_ratio_new']=['mean','min','max','var','skew']\n",
        "    \n",
        "    for col in ['category_2','category_3']:\n",
        "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
        "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
        "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
        "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
        "        aggs[col+'_mean'] = ['mean']\n",
        "\n",
        "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
        "\n",
        "    # change column name\n",
        "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
        "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
        "\n",
        "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max'] - new_merchant_df['new_purchase_date_min']).dt.days\n",
        "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff'] / new_merchant_df['new_card_id_size']\n",
        "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
        "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
        "\n",
        "    # reduce memory usage\n",
        "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    new_merchant_df.reset_index(inplace=True)\n",
        "    \n",
        "    # Re-construct 2019/2/19: add features from merchant data file\n",
        "#     data_merchant_agg = pd.read_csv(data_merchants_path)\n",
        "#     new_merchant_df = new_merchant_df.rename(columns={\"new_merchant_id_nunique\": \"merchant_id\"})\n",
        "#     new_merchant_df = aggregate_merchants(new_merchant_df, data_merchant_agg)\n",
        "#     print(\"Checking merge id for the next step: {} and {}\".format((\"merchant_id\" in new_merchant_df.columns), (\"merchant_id\" in data_merchant_agg.columns)))\n",
        "    # reduce memory usage\n",
        "#     new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "#     del  data_merchant_agg\n",
        "#     del new_merchant_df[\"merchant_id\"]\n",
        "    \n",
        "    return new_merchant_df\n",
        "\n",
        "\n",
        "# additional features\n",
        "def additional_features(df):\n",
        "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
        "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
        "\n",
        "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
        "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
        "\n",
        "    for f in date_features:\n",
        "        df[f] = df[f].astype(np.int64) * 1e-9\n",
        "\n",
        "    df['card_id_total'] = df['new_card_id_size'] + df['hist_card_id_size']\n",
        "    df['card_id_cnt_total'] = df['new_card_id_count'] + df['hist_card_id_count']\n",
        "    df['card_id_cnt_ratio'] = df['new_card_id_count'] / df['hist_card_id_count']\n",
        "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
        "    df['purchase_amount_mean'] = df['new_purchase_amount_mean'] + df['hist_purchase_amount_mean']\n",
        "    df['purchase_amount_max'] = df['new_purchase_amount_max'] + df['hist_purchase_amount_max']\n",
        "    df['purchase_amount_min'] = df['new_purchase_amount_min'] + df['hist_purchase_amount_min']\n",
        "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum'] / df['hist_purchase_amount_sum']\n",
        "    df['month_diff_mean'] = df['new_month_diff_mean'] + df['hist_month_diff_mean']\n",
        "    df['month_diff_ratio'] = df['new_month_diff_mean'] / df['hist_month_diff_mean']\n",
        "    df['month_lag_mean'] = df['new_month_lag_mean'] + df['hist_month_lag_mean']\n",
        "    df['month_lag_max'] = df['new_month_lag_max'] + df['hist_month_lag_max']\n",
        "    df['month_lag_min'] = df['new_month_lag_min'] + df['hist_month_lag_min']\n",
        "    df['category_1_mean'] = df['new_category_1_mean'] + df['hist_category_1_mean']\n",
        "    df['installments_total'] = df['new_installments_sum'] + df['hist_installments_sum']\n",
        "    df['installments_mean'] = df['new_installments_mean'] + df['hist_installments_mean']\n",
        "    df['installments_max'] = df['new_installments_max'] + df['hist_installments_max']\n",
        "    df['installments_ratio'] = df['new_installments_sum'] / df['hist_installments_sum']\n",
        "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
        "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
        "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
        "    df['duration_mean'] = df['new_duration_mean'] + df['hist_duration_mean']\n",
        "    df['duration_min'] = df['new_duration_min'] + df['hist_duration_min']\n",
        "    df['duration_max'] = df['new_duration_max'] + df['hist_duration_max']\n",
        "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean'] + df['hist_amount_month_ratio_mean']\n",
        "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min'] + df['hist_amount_month_ratio_min']\n",
        "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max'] + df['hist_amount_month_ratio_max']\n",
        "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
        "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
        "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
        "    \n",
        "    df = reduce_mem_usage(df)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fzf851BEaeCe"
      },
      "cell_type": "markdown",
      "source": [
        "### define model and model evaluation functions"
      ]
    },
    {
      "metadata": {
        "id": "17CPLilcEjQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display/plot feature importance\n",
        "def display_importances(feature_importance_df_):\n",
        "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
        "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
        "    plt.title('LightGBM Features (avg over folds)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./submit/lgbm_importances_20190219.png')\n",
        "    \n",
        "    return best_features\n",
        "\n",
        "    \n",
        "# LightGBM GBDT with KFold or Stratified KFold\n",
        "def kfold_lightgbm(train_df, test_df, num_folds, stratified=False, debug=False):\n",
        "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
        "\n",
        "    # Cross validation model\n",
        "    if stratified:\n",
        "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
        "    else:\n",
        "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
        "\n",
        "    # Create arrays and dataframes to store results\n",
        "    oof_preds = np.zeros(train_df.shape[0])\n",
        "    sub_preds = np.zeros(test_df.shape[0])\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
        "    \n",
        "    # Re-construct 2019/2/17: create dic to store loss\n",
        "    loss_train, loss_valid = {each_fold: [] for each_fold in range(num_folds)}, {each_fold: [] for each_fold in range(num_folds)}\n",
        "    \n",
        "    # k-fold\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
        "        \n",
        "        # set data structure\n",
        "        lgb_train = lgb.Dataset(train_x,\n",
        "                                label=train_y,\n",
        "                                free_raw_data=False)\n",
        "        lgb_test = lgb.Dataset(valid_x,\n",
        "                               label=valid_y,\n",
        "                               free_raw_data=False)\n",
        "\n",
        "        # params optimized by optuna\n",
        "        params ={\n",
        "                'task': 'train',\n",
        "                'boosting': 'goss',\n",
        "                'objective': 'regression',\n",
        "                'metric': 'rmse',\n",
        "                'learning_rate': 0.01,\n",
        "                'subsample': 0.9855232997390695,\n",
        "                'max_depth': 7,\n",
        "                'top_rate': 0.9064148448434349,\n",
        "                'num_leaves': 63,\n",
        "                'min_child_weight': 41.9612869171337,\n",
        "                'other_rate': 0.0721768246018207,\n",
        "                'reg_alpha': 9.677537745007898,\n",
        "                'colsample_bytree': 0.5665320670155495,\n",
        "                'min_split_gain': 9.820197773625843,\n",
        "                'reg_lambda': 8.2532317400459,\n",
        "                'min_data_in_leaf': 21,\n",
        "                'verbose': -1,\n",
        "                'seed':int(2**n_fold),\n",
        "                'bagging_seed':int(2**n_fold),\n",
        "                'drop_seed':int(2**n_fold)\n",
        "                }\n",
        "\n",
        "        reg = lgb.train(\n",
        "                        params,\n",
        "                        lgb_train,\n",
        "                        valid_sets=[lgb_train, lgb_test],\n",
        "                        valid_names=['train', 'test'],\n",
        "                        num_boost_round=10000,\n",
        "                        early_stopping_rounds= 200,\n",
        "                        verbose_eval=100\n",
        "                        )\n",
        "\n",
        "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
        "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
        "        gc.collect()\n",
        "        sys._clear_type_cache()\n",
        "        \n",
        "        # Re-construct 2019/2/17: store two kinds of loss        \n",
        "        loss_train[n_fold].append(rmse(oof_preds[train_idx], train_y[train_idx]))\n",
        "        loss_valid[n_fold].append(rmse(oof_preds[valid_idx], valid_y[valid_idx]))\n",
        "    \n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
        "        fold_importance_df[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
        "        del train_x, train_y, valid_x, valid_y\n",
        "        print(\"Display the importance range of fold {}: \".format(n_fold+1))\n",
        "        print(sorted(list(zip(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration), train_df.columns)), reverse=True))\n",
        "        del reg\n",
        "        gc.collect()\n",
        "        \n",
        "    # display importances\n",
        "    best_features_df = display_importances(feature_importance_df)\n",
        "\n",
        "    # Re-construct 2019/2/17: display final score\n",
        "    print('Final RMSE : %.6f' % (rmse(train_df['target'], oof_preds)))\n",
        " \n",
        "    # Re-construct 2019/2/16: evaluate modeling results\n",
        "#     with timer(\"Evaluate modeling results\"):\n",
        "#         best_features_df.set_index(\"fold\", inplace=True)\n",
        "#         best_features_df.loc[2, :].sort_values(by=\"importance\",ascending=False)\n",
        "        \n",
        "#         loss_train_df = pd.DataFrame(loss_train)\n",
        "#         loss_valid_df = pd.DataFrame(loss_valid)\n",
        "#         plt.plot(np.arange(loss_train_df.shape[1]), loss_train_df.loc[0, :].values, \"bo-\", np.arange(loss_valid_df.shape[1]), loss_valid_df.loc[0, :].values, \"ro-\");\n",
        "\n",
        "    if not debug:\n",
        "        # save submission file\n",
        "        \n",
        "        test_df.loc[:,'target'] = sub_preds\n",
        "        test_df = test_df.reset_index()\n",
        "        \n",
        "        test_df[['card_id', 'target']].to_csv(submission_file_name, index=False)\n",
        "        \n",
        "    return best_features_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Shw-7hxAaiXP"
      },
      "cell_type": "markdown",
      "source": [
        "### define execution functions"
      ]
    },
    {
      "metadata": {
        "id": "wZ5mbtm2EkDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "54e3bef3-cef2-4aa1-f518-402711dcc64e"
      },
      "cell_type": "code",
      "source": [
        "def main(debug=False):\n",
        "    num_rows = 10000 if debug else None\n",
        "    \n",
        "    with timer(\"train & test\"):\n",
        "        df = train_test(num_rows)\n",
        "        assert df.target.isnull().sum() == 123623\n",
        "        assert df.target.notnull().sum() == 201917\n",
        "#         print(df.shape)\n",
        "        \n",
        "    # Re-construct 2019/2/24: adding new count features extracted from new transaction file\n",
        "    with timer(\"UDF 2018/2/24\"):\n",
        "        new_trans_df_new1 = pd.read_csv(DATA_ROOT_PATH + \"data_raw/feats_new_20190222_part1.csv\")\n",
        "        new_trans_df_new2 = pd.read_csv(DATA_ROOT_PATH + \"data_raw/feats_new_20190222_part2.csv\")\n",
        "        new_trans_df_new1[\"card_id\"] =  new_trans_df_new2[\"card_id\"]    \n",
        "        df = pd.merge(df, new_trans_df_new1, on='card_id', how='left') \n",
        "        df = pd.merge(df, new_trans_df_new2, on='card_id', how='left') \n",
        "#         print(df.shape)\n",
        "        df = df.drop_duplicates(subset=\"card_id\")\n",
        "        assert df.target.isnull().sum() == 123623\n",
        "        assert df.target.notnull().sum() == 201917\n",
        "    \n",
        "    with timer(\"new merchants\"):\n",
        "        df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')    \n",
        "#         df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='left')    \n",
        "            \n",
        "    with timer(\"historical transactions\"):\n",
        "        df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')\n",
        "#         df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='left')\n",
        "    \n",
        "    with timer(\"additional features\"):\n",
        "        df = additional_features(df)\n",
        "            \n",
        "    with timer(\"split train & test\"):\n",
        "        train_df = df[df['target'].notnull()]\n",
        "        test_df = df[df['target'].isnull()]\n",
        "        \n",
        "        train_df.to_csv(DATA_ROOT_PATH + \"data_processed/train/data_train_20190224.csv\", index=False)\n",
        "        test_df.to_csv(DATA_ROOT_PATH + \"data_processed/test/data_test_20190224.csv\", index=False)\n",
        "        \n",
        "        del df\n",
        "        gc.collect()\n",
        "        \n",
        "    with timer(\"Run LightGBM with kfold\"):\n",
        "        best_features_df = kfold_lightgbm(train_df, test_df, num_folds=11, stratified=False, debug=debug)\n",
        "      \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    submission_file_name = './submit/submission_LGBM_hzq_20190224.csv'\n",
        "    with timer(\"Full model run\"):\n",
        "        main(debug=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train samples: 201917, test samples: 123623\n",
            "train & test - done in 10s\n",
            "UDF 2018/2/24 - done in 3s\n",
            "new merchants - done in 818s\n",
            "[Output] expected to be end in 5 runs, this is 1 run...\n",
            "[Output] expected to be end in 5 runs, this is 2 run...\n",
            "[Output] expected to be end in 5 runs, this is 3 run...\n",
            "[Output] expected to be end in 5 runs, this is 4 run...\n",
            "[Output] expected to be end in 5 runs, this is 5 run...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O00oozSBbU9I"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Parts"
      ]
    },
    {
      "metadata": {
        "id": "JH7FzZeEOqo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "sys._clear_type_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}