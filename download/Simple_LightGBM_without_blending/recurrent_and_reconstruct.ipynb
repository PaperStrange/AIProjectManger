{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kernel_recurrent_and_reconstruct.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "O00oozSBbU9I",
        "5bOAt6OnfSuu",
        "8Kvn9icnfXbp",
        "hdalc10ifXfh",
        "CLp3xweDfkG0",
        "_7gRpQLXQSID",
        "hVEG5D5TxFKW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "yAjXGPfiGxc7"
      },
      "cell_type": "markdown",
      "source": [
        "* Author: HZQ\n",
        "* Last modified: 2018/2/18\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vV5qe6BlG1_C",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools \n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null \n",
        "!apt-get update -qq 2>&1 > /dev/null \n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse \n",
        "from google.colab import auth \n",
        "auth.authenticate_user() \n",
        "from oauth2client.client import GoogleCredentials \n",
        "creds = GoogleCredentials.get_application_default() \n",
        "import getpass \n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL \n",
        "vcode = getpass.getpass() \n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "T0HVL61FHqMC",
        "outputId": "c6df39d8-114f-444f-f3ed-f7d3db72086b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p driver\n",
        "!google-drive-ocamlfuse driver\n",
        "import os\n",
        "\n",
        "os.chdir(\"driver/Colab Notebooks/CategoryRecom_kaggle/\")\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
            "data\t  driver   model      requirements.txt\ttimeline\n",
            "download  LICENSE  ReadMe.md  submit\t\tutils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8OwoC4PMaAbC",
        "outputId": "dd2119f3-d6f7-4b44-9a9d-746232c707d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "driver\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6EglTV4uGxdA"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Parts"
      ]
    },
    {
      "metadata": {
        "id": "M0wVXTxSzI9X",
        "colab_type": "code",
        "outputId": "96c89d08-d93c-4729-c929-14594e2addbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Used for better debug experience\n",
        "%xmode Plain\n",
        "# %pdb on"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Plain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qG85KhyNGxdK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import gc, sys\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
        "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
        "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
        "                  'OOF_PRED', 'month_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGXHmH-TFx0u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_ROOT_PATH = \"./data/\"\n",
        "\n",
        "data_train_path = DATA_ROOT_PATH + \"data_raw/train/train.csv\"\n",
        "# data_train_path = DATA_ROOT_PATH + \"data_raw/train/train_20190218.csv\"\n",
        "data_test_path = DATA_ROOT_PATH + \"data_raw/test/test.csv\"\n",
        "data_valid_path = DATA_ROOT_PATH + \"data_raw/valid/valid.csv\"\n",
        "data_info_path = DATA_ROOT_PATH  + \"data_raw/Data_Dictionary.xlsx\"\n",
        "data_merchants_path = DATA_ROOT_PATH + \"data_raw/merchants.csv\"\n",
        "# data_merchants_path = DATA_ROOT_PATH + \"data_raw/merchants_20190219.csv\"\n",
        "data_his_trans_path = DATA_ROOT_PATH + \"data_raw/historical_transactions_20190218.csv\"\n",
        "data_new_trans_path = DATA_ROOT_PATH + \"data_raw/new_merchant_transactions.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HENkr0SjG0Ff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# month_dict = \\\n",
        "#         {1:\"January\", \n",
        "#          2:\"February\", \n",
        "#          3:\"March\", \n",
        "#          4:\"April\", \n",
        "#          5:\"May\", \n",
        "#          6:\"June\", \n",
        "#          7:\"July\", \n",
        "#          8:\"August\", \n",
        "#          9:\"September\", \n",
        "#          10:\"October\", \n",
        "#          11:\"November\", \n",
        "#          12:\"December\"}\n",
        "\n",
        "@contextmanager\n",
        "def timer(title):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
        "\n",
        "    \n",
        "# rmse\n",
        "def rmse(y_true, y_pred):\n",
        "    \n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "\n",
        "# One-hot encoding for categorical columns with get_dummies\n",
        "def one_hot_encoder(df, nan_as_category = True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    \n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    return df, new_columns\n",
        "\n",
        "\n",
        "# reduce memory\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrNKOHJlIKm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1297
        },
        "outputId": "ad436a35-c0be-48b3-feb7-889231a4afc6"
      },
      "cell_type": "code",
      "source": [
        "test[test[\"card_id\"] == \"C_ID_0001238066\"]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorized_flag</th>\n",
              "      <th>card_id</th>\n",
              "      <th>city_id</th>\n",
              "      <th>category_1</th>\n",
              "      <th>installments</th>\n",
              "      <th>category_3</th>\n",
              "      <th>merchant_category_id</th>\n",
              "      <th>merchant_id</th>\n",
              "      <th>month_lag</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>purchase_date</th>\n",
              "      <th>category_2</th>\n",
              "      <th>state_id</th>\n",
              "      <th>subsector_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38616</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_fb8cc186f8</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.670423</td>\n",
              "      <td>2018-04-21 15:46:53</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38617</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>-1</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>650</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.700326</td>\n",
              "      <td>2018-03-24 11:22:13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38618</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>367</td>\n",
              "      <td>M_ID_235e546dcc</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.672136</td>\n",
              "      <td>2018-03-03 22:44:57</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38619</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>291</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_894b777cc9</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.718267</td>\n",
              "      <td>2018-04-23 09:19:45</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38620</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>-1</td>\n",
              "      <td>Y</td>\n",
              "      <td>6</td>\n",
              "      <td>C</td>\n",
              "      <td>771</td>\n",
              "      <td>M_ID_4635824091</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.559768</td>\n",
              "      <td>2018-04-16 20:04:56</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38621</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_84f1ff55b9</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.441148</td>\n",
              "      <td>2018-04-30 19:57:30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38622</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>248</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_af5fc8c340</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.596643</td>\n",
              "      <td>2018-03-18 16:45:27</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38623</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>-1</td>\n",
              "      <td>Y</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>839</td>\n",
              "      <td>M_ID_e5374dabc0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.740897</td>\n",
              "      <td>2018-03-16 12:03:12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38624</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>87</td>\n",
              "      <td>M_ID_b34bebaf3c</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.152008</td>\n",
              "      <td>2018-03-16 18:49:21</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38625</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>169</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_51b4a616bb</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.470360</td>\n",
              "      <td>2018-03-30 20:45:52</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38626</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_aaf127d072</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.579888</td>\n",
              "      <td>2018-04-11 22:45:50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38627</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>291</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_6b8228d6a6</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.656749</td>\n",
              "      <td>2018-04-14 13:21:53</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38628</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>333</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>607</td>\n",
              "      <td>M_ID_a5a61c543e</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.078318</td>\n",
              "      <td>2018-03-24 17:02:00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38629</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>288</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>80</td>\n",
              "      <td>M_ID_0bb734e74a</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.486949</td>\n",
              "      <td>2018-03-30 19:29:14</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38630</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_7d8102bb34</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.732783</td>\n",
              "      <td>2018-03-01 16:48:27</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38631</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>606</td>\n",
              "      <td>M_ID_cd2c0b07e9</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.688680</td>\n",
              "      <td>2018-03-24 17:36:19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38632</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>10</td>\n",
              "      <td>C</td>\n",
              "      <td>705</td>\n",
              "      <td>M_ID_edf884cae2</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.566770</td>\n",
              "      <td>2018-04-22 14:46:31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38633</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>333</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>783</td>\n",
              "      <td>M_ID_a88790a464</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.641722</td>\n",
              "      <td>2018-03-04 13:05:16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38634</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>333</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>367</td>\n",
              "      <td>M_ID_c9e1da1932</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.696494</td>\n",
              "      <td>2018-03-16 23:37:48</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38635</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>291</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>607</td>\n",
              "      <td>M_ID_e7a20356e3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.351786</td>\n",
              "      <td>2018-03-10 18:11:22</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38636</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>169</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>307</td>\n",
              "      <td>M_ID_a275791cd2</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.530526</td>\n",
              "      <td>2018-04-01 10:54:28</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38637</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_9962ea3faf</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.371245</td>\n",
              "      <td>2018-04-29 01:19:02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38638</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>291</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>278</td>\n",
              "      <td>M_ID_d855771cd9</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.694631</td>\n",
              "      <td>2018-03-22 09:20:18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38639</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>879</td>\n",
              "      <td>M_ID_00a6ca8a8a</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.676433</td>\n",
              "      <td>2018-03-30 00:18:46</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38640</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>69</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>68</td>\n",
              "      <td>M_ID_879f625c89</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.694315</td>\n",
              "      <td>2018-03-16 19:03:05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38641</th>\n",
              "      <td>Y</td>\n",
              "      <td>C_ID_0001238066</td>\n",
              "      <td>314</td>\n",
              "      <td>N</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>561</td>\n",
              "      <td>M_ID_ec24d672a3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.680791</td>\n",
              "      <td>2018-03-23 22:49:51</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      authorized_flag          card_id  city_id category_1  installments  \\\n",
              "38616               Y  C_ID_0001238066      314          N             1   \n",
              "38617               Y  C_ID_0001238066       -1          N             1   \n",
              "38618               Y  C_ID_0001238066      314          N             1   \n",
              "38619               Y  C_ID_0001238066      291          N             1   \n",
              "38620               Y  C_ID_0001238066       -1          Y             6   \n",
              "38621               Y  C_ID_0001238066      314          N             1   \n",
              "38622               Y  C_ID_0001238066      248          N             1   \n",
              "38623               Y  C_ID_0001238066       -1          Y             1   \n",
              "38624               Y  C_ID_0001238066       69          N            -1   \n",
              "38625               Y  C_ID_0001238066      169          N             1   \n",
              "38626               Y  C_ID_0001238066      314          N             1   \n",
              "38627               Y  C_ID_0001238066      291          N             1   \n",
              "38628               Y  C_ID_0001238066      333          N             3   \n",
              "38629               Y  C_ID_0001238066      288          N             1   \n",
              "38630               Y  C_ID_0001238066      314          N             1   \n",
              "38631               Y  C_ID_0001238066       69          N             1   \n",
              "38632               Y  C_ID_0001238066      314          N            10   \n",
              "38633               Y  C_ID_0001238066      333          N             1   \n",
              "38634               Y  C_ID_0001238066      333          N             1   \n",
              "38635               Y  C_ID_0001238066      291          N             3   \n",
              "38636               Y  C_ID_0001238066      169          N             1   \n",
              "38637               Y  C_ID_0001238066       69          N             1   \n",
              "38638               Y  C_ID_0001238066      291          N             1   \n",
              "38639               Y  C_ID_0001238066       69          N             1   \n",
              "38640               Y  C_ID_0001238066       69          N             1   \n",
              "38641               Y  C_ID_0001238066      314          N             1   \n",
              "\n",
              "      category_3  merchant_category_id      merchant_id  month_lag  \\\n",
              "38616          B                   278  M_ID_fb8cc186f8          2   \n",
              "38617          B                   650              NaN          1   \n",
              "38618          B                   367  M_ID_235e546dcc          1   \n",
              "38619          B                   278  M_ID_894b777cc9          2   \n",
              "38620          C                   771  M_ID_4635824091          2   \n",
              "38621          B                   278  M_ID_84f1ff55b9          2   \n",
              "38622          B                   278  M_ID_af5fc8c340          1   \n",
              "38623          B                   839  M_ID_e5374dabc0          1   \n",
              "38624        NaN                    87  M_ID_b34bebaf3c          1   \n",
              "38625          B                   307  M_ID_51b4a616bb          1   \n",
              "38626          B                   278  M_ID_aaf127d072          2   \n",
              "38627          B                   307  M_ID_6b8228d6a6          2   \n",
              "38628          C                   607  M_ID_a5a61c543e          1   \n",
              "38629          B                    80  M_ID_0bb734e74a          1   \n",
              "38630          B                   307  M_ID_7d8102bb34          1   \n",
              "38631          B                   606  M_ID_cd2c0b07e9          1   \n",
              "38632          C                   705  M_ID_edf884cae2          2   \n",
              "38633          B                   783  M_ID_a88790a464          1   \n",
              "38634          B                   367  M_ID_c9e1da1932          1   \n",
              "38635          C                   607  M_ID_e7a20356e3          1   \n",
              "38636          B                   307  M_ID_a275791cd2          2   \n",
              "38637          B                   278  M_ID_9962ea3faf          2   \n",
              "38638          B                   278  M_ID_d855771cd9          1   \n",
              "38639          B                   879  M_ID_00a6ca8a8a          1   \n",
              "38640          B                    68  M_ID_879f625c89          1   \n",
              "38641          B                   561  M_ID_ec24d672a3          1   \n",
              "\n",
              "       purchase_amount        purchase_date  category_2  state_id  \\\n",
              "38616        -0.670423  2018-04-21 15:46:53         1.0         9   \n",
              "38617        -0.700326  2018-03-24 11:22:13         NaN        -1   \n",
              "38618        -0.672136  2018-03-03 22:44:57         1.0         9   \n",
              "38619        -0.718267  2018-04-23 09:19:45         1.0         9   \n",
              "38620        -0.559768  2018-04-16 20:04:56         NaN        -1   \n",
              "38621        -0.441148  2018-04-30 19:57:30         1.0         9   \n",
              "38622        -0.596643  2018-03-18 16:45:27         1.0        15   \n",
              "38623        -0.740897  2018-03-16 12:03:12         NaN        -1   \n",
              "38624        -0.152008  2018-03-16 18:49:21         1.0         9   \n",
              "38625        -0.470360  2018-03-30 20:45:52         5.0        20   \n",
              "38626        -0.579888  2018-04-11 22:45:50         1.0         9   \n",
              "38627        -0.656749  2018-04-14 13:21:53         1.0         9   \n",
              "38628        -0.078318  2018-03-24 17:02:00         1.0         9   \n",
              "38629        -0.486949  2018-03-30 19:29:14         5.0        20   \n",
              "38630        -0.732783  2018-03-01 16:48:27         1.0         9   \n",
              "38631        -0.688680  2018-03-24 17:36:19         1.0         9   \n",
              "38632        -0.566770  2018-04-22 14:46:31         1.0         9   \n",
              "38633        -0.641722  2018-03-04 13:05:16         1.0         9   \n",
              "38634        -0.696494  2018-03-16 23:37:48         1.0         9   \n",
              "38635        -0.351786  2018-03-10 18:11:22         1.0         9   \n",
              "38636        -0.530526  2018-04-01 10:54:28         5.0        20   \n",
              "38637        -0.371245  2018-04-29 01:19:02         1.0         9   \n",
              "38638        -0.694631  2018-03-22 09:20:18         1.0         9   \n",
              "38639        -0.676433  2018-03-30 00:18:46         1.0         9   \n",
              "38640        -0.694315  2018-03-16 19:03:05         1.0         9   \n",
              "38641        -0.680791  2018-03-23 22:49:51         1.0         9   \n",
              "\n",
              "       subsector_id  \n",
              "38616            37  \n",
              "38617            29  \n",
              "38618            16  \n",
              "38619            37  \n",
              "38620            31  \n",
              "38621            37  \n",
              "38622            37  \n",
              "38623            29  \n",
              "38624            27  \n",
              "38625            19  \n",
              "38626            37  \n",
              "38627            19  \n",
              "38628            29  \n",
              "38629            37  \n",
              "38630            19  \n",
              "38631            17  \n",
              "38632            33  \n",
              "38633            19  \n",
              "38634            16  \n",
              "38635            29  \n",
              "38636            19  \n",
              "38637            37  \n",
              "38638            37  \n",
              "38639            29  \n",
              "38640            27  \n",
              "38641             7  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "gQDQSO0BDFTd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Re-construct 2019/2/17: fill vacancy merchant id by other method [here](https://www.kaggle.com/raddar/merchant-id-imputations)\n",
        "# def merchant_id_selected():\n",
        "#     new_merchant_df = pd.read_csv(data_new_trans_path).fillna('')\n",
        "#     fields = ['card_id','city_id','category_1','installments','category_3',\\\n",
        "#           'merchant_category_id','category_2','state_id','subsector_id']\n",
        "#     new_merchant_df_part = new_merchant_df[fields + ['merchant_id']].drop_duplicates()\n",
        "#     new_merchant_df_part = new_merchant_df_part.loc[new_merchant_df_part['merchant_id'] != '']  \n",
        "#     # take only unique merchants for the `fields` combination\n",
        "#     uq_new_merchants = new_merchant_df_part.groupby(fields)['merchant_id'].count().reset_index(name = 'n_merchants')\n",
        "#     uq_new_merchants = uq_new_merchants.loc[uq_new_merchants['n_merchants'] == 1]\n",
        "#     uq_new_merchants = uq_new_merchants.merge(new_merchant_df_part, on=fields)\n",
        "#     uq_new_merchants.drop('n_merchants', axis=1, inplace=True)\n",
        "#     # rename the merchant_id so we can join it more easily later on\n",
        "#     uq_new_merchants.columns = fields + ['imputed_merchant_id']\n",
        "    \n",
        "#     del new_merchant_df\n",
        "#     del new_merchant_df_part\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()    \n",
        "    \n",
        "#     return uq_new_merchants, fields\n",
        "\n",
        "\n",
        "# Re-construct 2019/2/17: fill vacancy merchant id by other method [here](https://www.kaggle.com/raddar/merchant-id-imputations)\n",
        "# def impute_his_merchant_id():\n",
        "#     hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)\n",
        "#     hist_list = []\n",
        "#     indent = 1    \n",
        "\n",
        "#     for each_chunk in hist_reader:\n",
        "#         print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "#         each_chunk = reduce_mem_usage(each_chunk)\n",
        "#         hist_list.append(each_chunk)\n",
        "#         indent += 1\n",
        "#         gc.collect()\n",
        "#     sys._clear_type_cache()        \n",
        "    \n",
        "#     hist_df = pd.concat(hist_list, ignore_index=True)\n",
        "#     del hist_list\n",
        "#     del each_chunk\n",
        "#     del hist_reader\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()  \n",
        "    \n",
        "#     # make the actual imputation for the merchant_id field   \n",
        "#     uq_new_merchants, fields = merchant_id_selected()  \n",
        "#     hist_df.fillna('', inplace=True) \n",
        "#     hist_df = hist_df.merge(uq_new_merchants, on = fields, how = 'left')\n",
        "#     index = (hist_df['merchant_id']=='') & (~pd.isnull(hist_df['imputed_merchant_id']))\n",
        "#     hist_df.loc[index, 'merchant_id'] = hist_df.loc[index, 'imputed_merchant_id']\n",
        "#     hist_df.drop([\"imputed_merchant_id\"], axis=1, inplace=True)\n",
        "#     hist_df.to_csv(DATA_ROOT_PATH + \"data_raw/historical_transactions_20190219.csv\", index=False)\n",
        "    \n",
        "#     return hist_df\n",
        "\n",
        "\n",
        "# hist_df = impute_his_merchant_id()\n",
        "\n",
        "\n",
        "# Re-construct 2019/2/17: use new retrieved target values\n",
        "# data_train = pd.read_csv(data_train_path)\n",
        "# data_train[\"target\"] = np.exp2(data_train[\"target\"]) - 0.0000000001\n",
        "# data_train_path = DATA_ROOT_PATH + \"data_raw/train/train_20190219.csv\"\n",
        "# data_train.to_csv(data_train_path)\n",
        "\n",
        "\n",
        "# # Re-construct 2019/2/19: design aggregation for merchant.csv\n",
        "# def aggregate_merchants(df, df_merchant):\n",
        "#     '''\n",
        "#     INPUT:\n",
        "#     df -  a pandas dataframe containing data after dealing with outliers, mixed feature\n",
        "#     df_merchant -  a specific pandas dataframe containing data of several merchants\n",
        "    \n",
        "#     OUTPUT:\n",
        "#     df_agg_merchant -  a pandas dataframe after aggregation from merchants csv file\n",
        "    \n",
        "#     This function applly specific aggregation to INPUT.df\n",
        "#     '''\n",
        "\n",
        "#     df_agg_merchant = pd.merge(df, df_merchant, on=\"merchant_id\", how=\"left\")    \n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()   \n",
        "    \n",
        "#     return df_agg_merchant\n",
        "\n",
        "\n",
        "# data_merchants= pd.read_csv(data_merchants_path)\n",
        "# data_merchants.drop([\"merchant_category_id\", \"city_id\", \"merchant_group_id\"], axis=1, inplace=True)\n",
        "# for each_col in ['category_1', 'category_4']:\n",
        "#     data_merchants[each_col] = data_merchants[each_col].map({\"Y\":1, \"N\":0})\n",
        "# data_merchants = pd.get_dummies(data_merchants, columns=[\"most_recent_sales_range\", \"most_recent_purchases_range\"])\n",
        "    \n",
        "# print(\"Excecute aggregation to VARIABLE data_merchants\")\n",
        "# print(\"[Input] Aggregate specific columns below\")\n",
        "# agg_func = {\n",
        "#         \"active_months_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"active_months_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"active_months_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_purchases_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag12\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag3\": [\"mean\", \"max\", \"min\"],\n",
        "# #         \"avg_sales_lag6\": [\"mean\", \"max\", \"min\"],\n",
        "#         \"category_1\": [\"sum\", \"mean\"],\n",
        "#         \"category_2\": [\"sum\", \"mean\"],\n",
        "#         \"category_4\": [\"sum\", \"mean\"],\n",
        "#         \"most_recent_purchases_range_A\": [\"mean\"],        \n",
        "#         \"most_recent_purchases_range_B\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_C\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_D\": [\"mean\"],    \n",
        "#         \"most_recent_purchases_range_E\": [\"mean\"],    \n",
        "#         \"most_recent_sales_range_A\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_B\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_C\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_D\": [\"mean\"], \n",
        "#         \"most_recent_sales_range_E\": [\"mean\"], \n",
        "#         \"numerical_1\": [\"sum\", \"mean\"],\n",
        "#         \"numerical_2\": [\"sum\", \"mean\"]\n",
        "#  }\n",
        "\n",
        "# data_merchants_agg = data_merchants.groupby([\"merchant_id\"]).agg(agg_func)\n",
        "# data_merchants_agg.reset_index(inplace=True)\n",
        "# data_merchants_agg.columns = [\"_\".join(col).strip() for col in data_merchants_agg.columns.values]\n",
        "# del data_merchants\n",
        "# gc.collect()\n",
        "# sys._clear_type_cache()\n",
        "\n",
        "# print(\"[Output] Store aggregated merchant data file: \")\n",
        "# temp = list(data_merchants_agg.columns)\n",
        "# temp[0] = \"merchant_id\"\n",
        "# data_merchants_agg.columns = temp\n",
        "# data_merchants_agg.to_csv( DATA_ROOT_PATH + \"data_raw/merchants_20190219.csv\", index=False)\n",
        "\n",
        "# Re-construct 2019/2/20: ADDITIONAL MONTHLY PURCHASE AMOUNT RATIO FEATURE  \n",
        "def groupby_row1(each_row1):\n",
        "    each_row1_part = each_row1.loc[:, [\"merchant_id\", \"month_lag\", \"purchase_amount\"]]\n",
        "    del each_row1\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    unique_merchant_ids = each_row1_part[\"merchant_id\"].drop_duplicates()\n",
        "    ratio_per_merchant_id = []\n",
        "    \n",
        "    for each_merchant in unique_merchant_ids:\n",
        "        temp = each_row1_part[each_row1_part[\"merchant_id\"] == each_merchant]\n",
        "        temp = temp.groupby(\"month_lag\").mean()\n",
        "#         print(\"After grouping: {}\".format(temp))\n",
        "        temp.reset_index(inplace=True)\n",
        "#         print(\"After re-indexing: {}\".format(temp))\n",
        "        gc.collect()\n",
        "        sys._clear_type_cache()\n",
        "        if temp.shape[0] == 1 or temp.shape[0] == 0:\n",
        "            ratio_per_merchant_id.append(0)\n",
        "        else:\n",
        "            print(\"Bingo!\")\n",
        "            ratio_per_merchant_id.append(\n",
        "                np.mean([\n",
        "                    np.power(\n",
        "                    each_row1_part.loc[i+1, \"purchase_amount\"] / each_row1_part.loc[i, \"purchase_amount\"], \n",
        "                    1.0 / (each_row1_part.loc[i+1, \"month_lag\"] - each_row1_part.loc[i, \"month_lag\"])\n",
        "                ) for i in range(each_row1_part.shape[0]-1)\n",
        "                            ])\n",
        "             )\n",
        "    \n",
        "    ratio_per_card_id = np.mean(ratio_per_merchant_id)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()    \n",
        "    print(\"Check result: {}\".format(ratio_per_card_id))\n",
        "        \n",
        "    return ratio_per_card_id          \n",
        "\n",
        "\n",
        "def purchase_amount_ratio(df):\n",
        "#     from collections import defaultdict\n",
        "#     unique_card_ids = df['card_id'].drop_duplicates()\n",
        "#     n = len(unique_card_ids)\n",
        "#     feature_df = pd.DataFrame({'card_id': unique_card_ids,\n",
        "#                                'monthly_purchase_amount_ratio': np.zeros(len(unique_card_ids))})\n",
        "\n",
        "#     for j, card_id in enumerate(unique_card_ids):\n",
        "#         card_df = df.loc[df['card_id'] == card_id, :]\n",
        "#         merchant_ids = df.loc[df['card_id'] == card_id, 'merchant_id']\n",
        "#         ratio = []\n",
        "#         print(len(merchant_ids))\n",
        "#         for merchant in merchant_ids:\n",
        "#             purchase_amount = card_df.loc[card_df['merchant_id'] == merchant, 'purchase_amount']\n",
        "#             month_lag = card_df.loc[card_df['merchant_id'] == merchant, 'month_lag']\n",
        "#             if len(purchase_amount):\n",
        "#                 unique_month_lag = month_lag.drop_duplicates()\n",
        "#                 pa_dict = defaultdict(float)\n",
        "#                 for i in range(len(purchase_amount)):\n",
        "#                     pa_dict[month_lag.values[i]] += purchase_amount.values[i]\n",
        "#                 m_ratio = []\n",
        "#                 s_lag = np.sort(unique_month_lag)\n",
        "#                 for i in range(len(unique_month_lag) - 1):\n",
        "#                     m_ratio.append(np.power(pa_dict[s_lag[i+1]] / pa_dict[s_lag[i]], 1.0 / (s_lag[i+1] - s_lag[i])))\n",
        "#                 if len(m_ratio):\n",
        "#                     ratio.append(np.mean(m_ratio))\n",
        "#         if len(ratio):\n",
        "#             feature_df.loc[feature_df['card_id'] == card_id, 'monthly_purchase_amount_ratio'] = np.mean(ratio)\n",
        "#         if (j+1) % 100 == 0 or j+1 == n:\n",
        "#             print('processed {}/{} unique card ids.'.format(j+1, n))\n",
        "\n",
        "    feature_df_new = df.groupby(\"card_id\").apply(groupby_row1)\n",
        "    assert df.groupby(\"card_id\").mean(),shape[0] == feature_df_new.shape[0]\n",
        "#     pd.MultiIndex.from_frame(df, names=None)\n",
        "    del df\n",
        "    \n",
        "    return feature_df_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vsdcNW0Zdalf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4751
        },
        "outputId": "f26217c2-c618-46ff-8cc6-d3962474abe8"
      },
      "cell_type": "code",
      "source": [
        "# test = pd.read_csv(data_new_trans_path)\n",
        "# feature_df = purchase_amount_ratio(test)\n",
        "purchase_amount_ratio(test)\n",
        "\n",
        "# hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)\n",
        "# hist_list = []\n",
        "# indent = 1\n",
        "# for each_chunk in hist_reader:\n",
        "#     print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "#     each_chunk = reduce_mem_usage(each_chunk)      \n",
        "#     hist_list.append(each_chunk)\n",
        "#     indent += 1\n",
        "#     gc.collect()\n",
        "# sys._clear_type_cache()        \n",
        "    \n",
        "# hist_df = pd.concat(hist_list, ignore_index=True)\n",
        "# del hist_list\n",
        "# del each_chunk\n",
        "# del hist_reader\n",
        "# gc.collect()\n",
        "# sys._clear_type_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n",
            "Check result: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gB3M2g3MESwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO 2019/2/18: MIDfc7d7969c3 = Netflix subscription prices\n",
        "\n",
        "\n",
        "# preprocessing train & test\n",
        "def train_test(num_rows=None):\n",
        "\n",
        "    # load csv\n",
        "    train_df = pd.read_csv(data_train_path, index_col=['card_id'], nrows=num_rows)\n",
        "    test_df = pd.read_csv(data_test_path, index_col=['card_id'], nrows=num_rows)\n",
        "\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
        "\n",
        "    # outlier\n",
        "    train_df['outliers'] = 0\n",
        "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
        "    \n",
        "    # Re-construct: drop outliers\n",
        "#     index_outliers = (train_df.loc[train_df[\"outliers\"] == 1, :]).index\n",
        "#     train_df.drop(index_outliers, axis=0, inplace=True)\n",
        "#     print(index_outliers)\n",
        "\n",
        "    # set target as nan\n",
        "    test_df['target'] = np.nan\n",
        "\n",
        "    # merge\n",
        "    df = train_df.append(test_df)\n",
        "\n",
        "    del train_df, test_df\n",
        "    gc.collect()\n",
        "\n",
        "    # to datetime\n",
        "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
        "\n",
        "    # datetime features\n",
        "    df['quarter'] = df['first_active_month'].dt.quarter\n",
        "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
        "    \n",
        "    # Re-construct: add new elasp startoff\n",
        "#     df[\"elasped_time_sp\"] = (datetime.date(2018, 2, 1) - df['first_active_month']).dt.days\n",
        "    \n",
        "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
        "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
        "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
        "\n",
        "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
        "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
        "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
        "\n",
        "    # one hot encoding\n",
        "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
        "\n",
        "    for f in ['feature_1','feature_2','feature_3']:\n",
        "        order_label = df.groupby([f])['outliers'].mean()\n",
        "        df[f] = df[f].map(order_label)\n",
        "\n",
        "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
        "    df['feature_mean'] = df['feature_sum'] / 3\n",
        "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
        "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
        "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
        "    \n",
        "    df = reduce_mem_usage(df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# preprocessing historical transactions\n",
        "def historical_transactions(num_rows=None):\n",
        "    # load csv\n",
        "    hist_reader = pd.read_csv(data_his_trans_path, chunksize=6000000)\n",
        "    hist_list = []\n",
        "    indent = 1\n",
        "    for each_chunk in hist_reader:\n",
        "        print(\"[Output] expected to be end in 5 runs, this is {} run...\".format(indent))\n",
        "        each_chunk = reduce_mem_usage(each_chunk)      \n",
        "        hist_list.append(each_chunk)\n",
        "        indent += 1\n",
        "        gc.collect()\n",
        "    sys._clear_type_cache()        \n",
        "    \n",
        "    hist_df = pd.concat(hist_list, ignore_index=True)\n",
        "    del hist_list\n",
        "    del each_chunk\n",
        "    del hist_reader\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()   \n",
        "    \n",
        "    # fillna    \n",
        "#     hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)     \n",
        "    hist_df['category_2'].fillna(1.0, inplace=True)\n",
        "    hist_df['category_3'].fillna('A', inplace=True)\n",
        "    hist_df['installments'].replace(-1, np.nan, inplace=True)\n",
        "    hist_df['installments'].replace(999, np.nan, inplace=True)\n",
        "  \n",
        "    # Re-construct 2019/2/17: create new \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
        "    hist_df[\"purchase_amount\"] = np.round(hist_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
        "    \n",
        "    # trim\n",
        "#     hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "    # Y/N to 1/0\n",
        "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
        "\n",
        "    # datetime features\n",
        "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
        "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
        "    \n",
        "    # Re-construct 2019/2/18: create categorical data for each month     \n",
        "#     hist_df[\"month_session\"] = hist_df[\"month\"].map(month_dict)\n",
        "#     hist_df = pd.get_dummies(hist_df, columns=[\"month_session\"], drop_first=False)\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()\n",
        "    # Or:\n",
        "#     month_session_cols_list = [\"month_session\"+str(ind+1) for ind in range(12)]\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "#     temp = pd.DataFrame(data=le.fit_transform(hist_df[\"month_session\"]), columns=month_session_cols_list)\n",
        "#     hist_df = pd.concat([hist_df, temp], ignore_index=True)\n",
        "#     print(hist_df.isnull())\n",
        "    \n",
        "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
        "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
        "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
        "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
        "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
        "\n",
        "    # additional features\n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
        "    \n",
        "    #Christmas : December 25 2017\n",
        "    hist_df['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Mothers Day: May 14 2017\n",
        "    hist_df['Mothers_Day_2017'] = (pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #fathers day: August 13 2017\n",
        "    hist_df['fathers_day_2017'] = (pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Childrens day: October 12 2017\n",
        "    hist_df['Children_day_2017'] = (pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Valentine's Day : 12th June, 2017\n",
        "    hist_df['Valentine_Day_2017'] = (pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Black Friday : 24th November 2017\n",
        "    hist_df['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    #2018\n",
        "    #Mothers Day: May 13 2018\n",
        "    hist_df['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
        "    hist_df['month_diff'] += hist_df['month_lag']\n",
        "\n",
        "    # additional features \n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    hist_df['duration'] = hist_df['purchase_amount'] * hist_df['month_diff']\n",
        "    hist_df['amount_month_ratio'] = hist_df['purchase_amount'] / hist_df['month_diff']\n",
        "    \n",
        "    # reduce memory usage\n",
        "    hist_df = reduce_mem_usage(hist_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
        "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
        "\n",
        "    aggs = {}\n",
        "    for col in col_unique:\n",
        "        aggs[col] = ['nunique']\n",
        "\n",
        "    for col in col_seas:\n",
        "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
        "\n",
        "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
        "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
        "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
        "    aggs['purchase_date'] = ['max','min']\n",
        "    \n",
        "    # Re-construct 2019/2/19: add month aggregation     \n",
        "    aggs['month'] = ['max','min','mean','var','skew']\n",
        "#     aggs[\"month_session_April\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_August\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_December\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_February\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_January\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_July\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_June\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_March\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_May\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_November\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_October\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_September\"] = [\"mean\"]\n",
        "    \n",
        "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
        "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
        "    aggs['authorized_flag'] = ['mean']\n",
        "    aggs['weekend'] = ['mean'] # overwrite\n",
        "    aggs['weekday'] = ['mean'] # overwrite\n",
        "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
        "    aggs['category_1'] = ['mean']\n",
        "    aggs['category_2'] = ['mean']\n",
        "    aggs['category_3'] = ['mean']\n",
        "    aggs['card_id'] = ['size','count']\n",
        "    aggs['price'] = ['sum','mean','max','min','var']\n",
        "#     aggs['price_new'] = ['sum','mean','max','min','var']    \n",
        "    aggs['Christmas_Day_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2017'] = ['mean']\n",
        "    aggs['fathers_day_2017'] = ['mean']\n",
        "    aggs['Children_day_2017'] = ['mean']\n",
        "    aggs['Valentine_Day_2017'] = ['mean']\n",
        "    aggs['Black_Friday_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2018'] = ['mean']\n",
        "    aggs['duration']=['mean','min','max','var','skew']\n",
        "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
        "#     aggs['duration_new']=['mean','min','max','var','skew']\n",
        "#     aggs['amount_month_ratio_new']=['mean','min','max','var','skew']\n",
        "\n",
        "    for col in ['category_2','category_3']:\n",
        "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
        "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
        "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
        "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
        "        aggs[col+'_mean'] = ['mean']\n",
        "\n",
        "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
        "\n",
        "    # change column name\n",
        "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
        "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
        "\n",
        "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max'] - hist_df['hist_purchase_date_min']).dt.days\n",
        "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff'] / hist_df['hist_card_id_size']\n",
        "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
        "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
        "\n",
        "    # reduce memory usage\n",
        "    hist_df = reduce_mem_usage(hist_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    hist_df.reset_index(inplace=True)\n",
        "    \n",
        "    return hist_df\n",
        " \n",
        "    \n",
        "# preprocessing new_merchant_transactions\n",
        "def new_merchant_transactions(num_rows=None):\n",
        "    # load csv\n",
        "    new_merchant_df = pd.read_csv(data_new_trans_path, nrows=num_rows)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # fillna\n",
        "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
        "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
        "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
        "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
        "    \n",
        "    # Re-construct 2019/2/17: create new \"purchase_amount\" referred [here](https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights)\n",
        "    new_merchant_df[\"purchase_amount\"] = np.round(new_merchant_df[\"purchase_amount\"] / 0.00150265118 + 497.06,2)\n",
        "    \n",
        "    # trim\n",
        "#     new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "    # Y/N to 1/0\n",
        "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
        "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
        "\n",
        "    # datetime features\n",
        "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
        "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
        "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
        "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
        "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
        "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
        "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
        "\n",
        "    # Re-construct 2019/2/18: create categorical data for each month     \n",
        "#     new_merchant_df[\"month_session\"] = new_merchant_df[\"month\"].map(month_dict)\n",
        "#     new_merchant_df = pd.get_dummies(new_merchant_df, columns=[\"month_session\"], drop_first=False)\n",
        "#     gc.collect()\n",
        "#     sys._clear_type_cache()\n",
        "    # Or:\n",
        "#     month_session_cols_list = [\"month_session\"+str(ind+1) for ind in range(12)]\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "#     temp = pd.DataFrame(data=le.fit_transform(new_merchant_df[\"month_session\"]), columns=month_session_cols_list)\n",
        "#     new_merchant_df = pd.concat([new_merchant_df, temp], ignore_index=True)\n",
        "#     print(new_merchant_df.isnull())\n",
        "    \n",
        "    # additional features \n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']     \n",
        "    \n",
        "    #Christmas : December 25 2017\n",
        "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Childrens day: October 12 2017\n",
        "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "    #Black Friday : 24th November 2017\n",
        "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    #Mothers Day: May 13 2018\n",
        "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
        "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
        "\n",
        "    # additional features\n",
        "    # Re-construct 2019/2/18: create new feature from \"purchase_amount\"\n",
        "    new_merchant_df['duration'] = new_merchant_df['purchase_amount'] * new_merchant_df['month_diff']\n",
        "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount'] / new_merchant_df['month_diff']\n",
        "    \n",
        "    # reduce memory usage\n",
        "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "    \n",
        "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
        "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
        "\n",
        "    aggs = {}\n",
        "    for col in col_unique:\n",
        "        aggs[col] = ['nunique']\n",
        "\n",
        "    for col in col_seas:\n",
        "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
        "\n",
        "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
        "#     aggs['purchase_amount_new'] = ['sum','max','min','mean','var','skew']\n",
        "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
        "    aggs['purchase_date'] = ['max','min']\n",
        "    \n",
        "    # Re-construct 2019/2/19: add month aggregation     \n",
        "    aggs['month'] = ['max','min','mean','var','skew']\n",
        "#     aggs[\"month_session_April\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_August\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_December\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_February\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_January\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_July\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_June\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_March\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_May\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_November\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_October\"] = [\"mean\"]\n",
        "#     aggs[\"month_session_September\"] = [\"mean\"]\n",
        "        \n",
        "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
        "    aggs['month_diff'] = ['mean','var','skew']\n",
        "    aggs['weekend'] = ['mean']\n",
        "    aggs['month'] = ['mean', 'min', 'max']\n",
        "    aggs['weekday'] = ['mean', 'min', 'max']\n",
        "    aggs['category_1'] = ['mean']\n",
        "    aggs['category_2'] = ['mean']\n",
        "    aggs['category_3'] = ['mean']\n",
        "    aggs['card_id'] = ['size','count']\n",
        "    aggs['price'] = ['mean','max','min','var']\n",
        "#     aggs['price_new'] = ['sum','mean','max','min','var']\n",
        "    aggs['Christmas_Day_2017'] = ['mean']\n",
        "    aggs['Children_day_2017'] = ['mean']\n",
        "    aggs['Black_Friday_2017'] = ['mean']\n",
        "    aggs['Mothers_Day_2018'] = ['mean']\n",
        "    aggs['duration']=['mean','min','max','var','skew']\n",
        "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
        "#     aggs['duration_new']=['mean','min','max','var','skew']\n",
        "#     aggs['amount_month_ratio_new']=['mean','min','max','var','skew']\n",
        "    \n",
        "    for col in ['category_2','category_3']:\n",
        "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
        "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
        "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
        "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
        "        aggs[col+'_mean'] = ['mean']\n",
        "\n",
        "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
        "\n",
        "    # change column name\n",
        "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
        "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
        "\n",
        "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max'] - new_merchant_df['new_purchase_date_min']).dt.days\n",
        "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff'] / new_merchant_df['new_card_id_size']\n",
        "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
        "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
        "\n",
        "    # reduce memory usage\n",
        "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "    gc.collect()\n",
        "    sys._clear_type_cache()\n",
        "\n",
        "    # Re-construct 2019/2/17: solve the KeyError: \"card_id\"\n",
        "    new_merchant_df.reset_index(inplace=True)\n",
        "    \n",
        "    # Re-construct 2019/2/19: add features from merchant data file\n",
        "#     data_merchant_agg = pd.read_csv(data_merchants_path)\n",
        "#     new_merchant_df = new_merchant_df.rename(columns={\"new_merchant_id_nunique\": \"merchant_id\"})\n",
        "#     new_merchant_df = aggregate_merchants(new_merchant_df, data_merchant_agg)\n",
        "#     print(\"Checking merge id for the next step: {} and {}\".format((\"merchant_id\" in new_merchant_df.columns), (\"merchant_id\" in data_merchant_agg.columns)))\n",
        "    # reduce memory usage\n",
        "#     new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
        "#     del  data_merchant_agg\n",
        "#     del new_merchant_df[\"merchant_id\"]\n",
        "    \n",
        "    return new_merchant_df\n",
        "\n",
        "\n",
        "# additional features\n",
        "def additional_features(df):\n",
        "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
        "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
        "\n",
        "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
        "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
        "\n",
        "    for f in date_features:\n",
        "        df[f] = df[f].astype(np.int64) * 1e-9\n",
        "\n",
        "    df['card_id_total'] = df['new_card_id_size'] + df['hist_card_id_size']\n",
        "    df['card_id_cnt_total'] = df['new_card_id_count'] + df['hist_card_id_count']\n",
        "    df['card_id_cnt_ratio'] = df['new_card_id_count'] / df['hist_card_id_count']\n",
        "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
        "    df['purchase_amount_mean'] = df['new_purchase_amount_mean'] + df['hist_purchase_amount_mean']\n",
        "    df['purchase_amount_max'] = df['new_purchase_amount_max'] + df['hist_purchase_amount_max']\n",
        "    df['purchase_amount_min'] = df['new_purchase_amount_min'] + df['hist_purchase_amount_min']\n",
        "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum'] / df['hist_purchase_amount_sum']\n",
        "    df['month_diff_mean'] = df['new_month_diff_mean'] + df['hist_month_diff_mean']\n",
        "    df['month_diff_ratio'] = df['new_month_diff_mean'] / df['hist_month_diff_mean']\n",
        "    df['month_lag_mean'] = df['new_month_lag_mean'] + df['hist_month_lag_mean']\n",
        "    df['month_lag_max'] = df['new_month_lag_max'] + df['hist_month_lag_max']\n",
        "    df['month_lag_min'] = df['new_month_lag_min'] + df['hist_month_lag_min']\n",
        "    df['category_1_mean'] = df['new_category_1_mean'] + df['hist_category_1_mean']\n",
        "    df['installments_total'] = df['new_installments_sum'] + df['hist_installments_sum']\n",
        "    df['installments_mean'] = df['new_installments_mean'] + df['hist_installments_mean']\n",
        "    df['installments_max'] = df['new_installments_max'] + df['hist_installments_max']\n",
        "    df['installments_ratio'] = df['new_installments_sum'] / df['hist_installments_sum']\n",
        "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
        "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
        "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
        "    df['duration_mean'] = df['new_duration_mean'] + df['hist_duration_mean']\n",
        "    df['duration_min'] = df['new_duration_min'] + df['hist_duration_min']\n",
        "    df['duration_max'] = df['new_duration_max'] + df['hist_duration_max']\n",
        "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean'] + df['hist_amount_month_ratio_mean']\n",
        "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min'] + df['hist_amount_month_ratio_min']\n",
        "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max'] + df['hist_amount_month_ratio_max']\n",
        "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
        "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
        "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
        "    \n",
        "    df = reduce_mem_usage(df)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17CPLilcEjQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display/plot feature importance\n",
        "def display_importances(feature_importance_df_):\n",
        "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
        "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
        "    plt.title('LightGBM Features (avg over folds)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./submit/lgbm_importances_20190219.png')\n",
        "    \n",
        "    return best_features\n",
        "\n",
        "    \n",
        "# LightGBM GBDT with KFold or Stratified KFold\n",
        "def kfold_lightgbm(train_df, test_df, num_folds, stratified=False, debug=False):\n",
        "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
        "\n",
        "    # Cross validation model\n",
        "    if stratified:\n",
        "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
        "    else:\n",
        "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
        "\n",
        "    # Create arrays and dataframes to store results\n",
        "    oof_preds = np.zeros(train_df.shape[0])\n",
        "    sub_preds = np.zeros(test_df.shape[0])\n",
        "    feature_importance_df = pd.DataFrame()\n",
        "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
        "    \n",
        "    # Re-construct 2019/2/17: create dic to store loss\n",
        "    loss_train, loss_valid = {each_fold: [] for each_fold in range(num_folds)}, {each_fold: [] for each_fold in range(num_folds)}\n",
        "    \n",
        "    # k-fold\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
        "        \n",
        "        # set data structure\n",
        "        lgb_train = lgb.Dataset(train_x,\n",
        "                                label=train_y,\n",
        "                                free_raw_data=False)\n",
        "        lgb_test = lgb.Dataset(valid_x,\n",
        "                               label=valid_y,\n",
        "                               free_raw_data=False)\n",
        "\n",
        "        # params optimized by optuna\n",
        "        params ={\n",
        "                'task': 'train',\n",
        "                'boosting': 'goss',\n",
        "                'objective': 'regression',\n",
        "                'metric': 'rmse',\n",
        "                'learning_rate': 0.01,\n",
        "                'subsample': 0.9855232997390695,\n",
        "                'max_depth': 7,\n",
        "                'top_rate': 0.9064148448434349,\n",
        "                'num_leaves': 63,\n",
        "                'min_child_weight': 41.9612869171337,\n",
        "                'other_rate': 0.0721768246018207,\n",
        "                'reg_alpha': 9.677537745007898,\n",
        "                'colsample_bytree': 0.5665320670155495,\n",
        "                'min_split_gain': 9.820197773625843,\n",
        "                'reg_lambda': 8.2532317400459,\n",
        "                'min_data_in_leaf': 21,\n",
        "                'verbose': -1,\n",
        "                'seed':int(2**n_fold),\n",
        "                'bagging_seed':int(2**n_fold),\n",
        "                'drop_seed':int(2**n_fold)\n",
        "                }\n",
        "\n",
        "        reg = lgb.train(\n",
        "                        params,\n",
        "                        lgb_train,\n",
        "                        valid_sets=[lgb_train, lgb_test],\n",
        "                        valid_names=['train', 'test'],\n",
        "                        num_boost_round=10000,\n",
        "                        early_stopping_rounds= 200,\n",
        "                        verbose_eval=100\n",
        "                        )\n",
        "\n",
        "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
        "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
        "        gc.collect()\n",
        "        sys._clear_type_cache()\n",
        "        \n",
        "        # Re-construct 2019/2/17: store two kinds of loss        \n",
        "        loss_train[n_fold].append(rmse(oof_preds[train_idx], train_y[train_idx]))\n",
        "        loss_valid[n_fold].append(rmse(oof_preds[valid_idx], valid_y[valid_idx]))\n",
        "    \n",
        "        fold_importance_df = pd.DataFrame()\n",
        "        fold_importance_df[\"feature\"] = feats\n",
        "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
        "        fold_importance_df[\"fold\"] = n_fold + 1\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
        "        del train_x, train_y, valid_x, valid_y\n",
        "        print(\"Display the importance range of fold {}: \".format(n_fold+1))\n",
        "        print(sorted(list(zip(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration), train_df.columns)), reverse=True))\n",
        "        del reg\n",
        "        gc.collect()\n",
        "        \n",
        "    # display importances\n",
        "    best_features_df = display_importances(feature_importance_df)\n",
        "\n",
        "    # Re-construct 2019/2/17: display final score\n",
        "    print('Final RMSE : %.6f' % (rmse(train_df['target'], oof_preds)))\n",
        " \n",
        "    # Re-construct 2019/2/16: evaluate modeling results\n",
        "#     with timer(\"Evaluate modeling results\"):\n",
        "#         best_features_df.set_index(\"fold\", inplace=True)\n",
        "#         best_features_df.loc[2, :].sort_values(by=\"importance\",ascending=False)\n",
        "        \n",
        "#         loss_train_df = pd.DataFrame(loss_train)\n",
        "#         loss_valid_df = pd.DataFrame(loss_valid)\n",
        "#         plt.plot(np.arange(loss_train_df.shape[1]), loss_train_df.loc[0, :].values, \"bo-\", np.arange(loss_valid_df.shape[1]), loss_valid_df.loc[0, :].values, \"ro-\");\n",
        "\n",
        "    if not debug:\n",
        "        # save submission file\n",
        "        \n",
        "        test_df.loc[:,'target'] = sub_preds\n",
        "        test_df = test_df.reset_index()\n",
        "        \n",
        "        test_df[['card_id', 'target']].to_csv(submission_file_name, index=False)\n",
        "        \n",
        "    return best_features_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZ5mbtm2EkDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(debug=False):\n",
        "    num_rows = 10000 if debug else None\n",
        "    \n",
        "    with timer(\"train & test\"):\n",
        "        df = train_test(num_rows)\n",
        "        print(df.target.isnull().sum())\n",
        "#         assert df.target.isnull().sum() == \n",
        "        \n",
        "    # Re-construct 2019/2/19: exchange location between two transaction data file dealing process\n",
        "    with timer(\"new merchants\"):\n",
        "        df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')    \n",
        "#         df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='left')    \n",
        "#         print(df.target.isnull().sum())\n",
        "            \n",
        "    with timer(\"historical transactions\"):\n",
        "        df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')\n",
        "#         df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='left')\n",
        "#         print(df.target.isnull().sum())\n",
        "    \n",
        "    with timer(\"additional features\"):\n",
        "        df = additional_features(df)\n",
        "#         print(df.target.isnull().sum())\n",
        "            \n",
        "    with timer(\"split train & test\"):\n",
        "        train_df = df[df['target'].notnull()]\n",
        "        test_df = df[df['target'].isnull()]\n",
        "        \n",
        "#         train_df.to_csv(DATA_ROOT_PATH + \"data_processed/train/data_train_20190219.csv\", index=False)\n",
        "#         test_df.to_csv(DATA_ROOT_PATH + \"data_processed/test/data_test_20190219.csv\", index=False)\n",
        "        \n",
        "        del df\n",
        "        gc.collect()\n",
        "        \n",
        "    with timer(\"Run LightGBM with kfold\"):\n",
        "        best_features_df = kfold_lightgbm(train_df, test_df, num_folds=11, stratified=False, debug=debug)\n",
        "      \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    submission_file_name = './submit/submission_LGBM_hzq_20190219.csv'\n",
        "    with timer(\"Full model run\"):\n",
        "        main(debug=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "O00oozSBbU9I"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Parts"
      ]
    },
    {
      "metadata": {
        "id": "JH7FzZeEOqo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "sys._clear_type_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bOAt6OnfSuu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## for data"
      ]
    },
    {
      "metadata": {
        "id": "LE3Wt4xZGEXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# calculate factorial\n",
        "\n",
        "\n",
        "from functools import reduce\n",
        "result = reduce(lambda x,y:x*y,range(1,num_stop+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Kvn9icnfXbp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### by pandas"
      ]
    },
    {
      "metadata": {
        "id": "b0PFUSPXvW_D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df..plot.scatter/bar/..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J73GtPtTf0I-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.isnull.sum().mean()  # provide a pandas series of the percentage of each row vacancy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KtWdPTXkf8bF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columnA.value_counts()  # provide a pandas series of the counts for a specified column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeIBxqxk6TQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[\"Sample_mod\"] = df[\"Sample\"].apply(func_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhkFGei176OF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.style.bar(subset=[\"Sample\"], align=\"mid\", color=[\"#d65f5f\", \"#5fba7d\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZEiJmStFGIJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "month_labels = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "                'September', 'October', 'November', 'December']\n",
        "temp['purchase_month'] = pd.Categorical(temp['purchase_month'], categories = month_labels, \n",
        "                                          ordered = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BHqq0jo1uD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_obj = df.select_dtypes(include=['object']).copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KWg6axHb0lXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fill_mode = lambda col: col.fillna(col.mode()[0])\n",
        "\n",
        "new_df.apply(fill_mode, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdalc10ifXfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### by matplotlib"
      ]
    },
    {
      "metadata": {
        "id": "93ovAnFNQC-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2,sharex=True)\n",
        "fig.subplots_adjust(hspace=.5)\n",
        "axes[0].hist(self.data, density=True)\n",
        "axes[0].set_title('Normed Histogram of Data')\n",
        "axes[0].set_ylabel('Density')\n",
        "\n",
        "axes[1].plot(x, y)\n",
        "axes[1].set_title('Normal Distribution for \\n Sample Mean and Sample Standard Deviation')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CLp3xweDfkG0"
      },
      "cell_type": "markdown",
      "source": [
        "## for algorithm or package"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IRkYB7UnGxdu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tushare as ts\n",
        "\n",
        "\n",
        "data_stock = ts.get_hist_data(\"600519\")\n",
        "data_stock.isna().sum()\n",
        "# data_stock.index\n",
        "print(data_stock.columns, type(data_stock))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_7gRpQLXQSID"
      },
      "cell_type": "markdown",
      "source": [
        "## Table Forms\n",
        "\n",
        "Forms provide an easy way to parameterize code. From a code cell, select **Insert → Add form field**.  When you change the value in a form, the corresponding value in the code will change. "
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "3jKM6GfzlgpS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title String fields\n",
        "\n",
        "text = 'value' #@param {type:\"string\"}\n",
        "dropdown = '1st option' #@param [\"1st option\", \"2nd option\", \"3rd option\"]\n",
        "text_and_dropdown = 'value' #@param [\"1st option\", \"2nd option\", \"3rd option\"] {allow-input: true}\n",
        "\n",
        "print(text)\n",
        "print(dropdown)\n",
        "print(text_and_dropdown)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "bf5LUmgZt-kT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Raw fields\n",
        "\n",
        "raw_input = None #@param {type:\"raw\"}\n",
        "raw_dropdown = raw_input #@param [1, \"raw_input\", \"False\", \"'string'\"] {type:\"raw\"}\n",
        "\n",
        "print(raw_input)\n",
        "print(raw_dropdown)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "aw5lgeRbubeF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Date fields\n",
        "date_input = '2018-03-22' #@param {type:\"date\"}\n",
        "\n",
        "print(date_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "eFN7-fUKs-Bu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Number fields\n",
        "number_input = 10.0 #@param {type:\"number\"}\n",
        "number_slider = 0 #@param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "\n",
        "integer_input = 10 #@param {type:\"integer\"}\n",
        "integer_slider = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "print(number_input)\n",
        "print(number_slider)\n",
        "\n",
        "print(integer_input)\n",
        "print(integer_slider)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "ig8PIYeLtM8g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Boolean fields\n",
        "boolean_checkbox = True #@param {type:\"boolean\"}\n",
        "boolean_dropdown = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "print(boolean_checkbox)\n",
        "print(boolean_dropdown)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hVEG5D5TxFKW"
      },
      "cell_type": "markdown",
      "source": [
        "### Hiding code\n",
        "\n",
        "You can change the view of the form by selecting **Edit → Show/hide code** or using the toolbar above the selected code cell. You can see both code and the form, just the form, or just the code."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZCEBZPwUDGOg",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title ## Markdown\n",
        "#@markdown You can also include Markdown in forms.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter a file path:\n",
        "file_path = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J9Thn2jvYQLI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}